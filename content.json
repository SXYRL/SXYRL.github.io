{"meta":{"title":"闫瑞龙的个人博客","subtitle":null,"description":null,"author":"闫瑞龙","url":"http://yoursite.com"},"pages":[{"title":"","date":"2018-02-28T13:41:16.976Z","updated":"2018-02-28T13:41:16.976Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-02-28T13:40:53.000Z","updated":"2018-02-28T13:40:53.025Z","comments":true,"path":"categories/index1.html","permalink":"http://yoursite.com/categories/index1.html","excerpt":"","text":""},{"title":"tags","date":"2018-02-28T13:39:07.000Z","updated":"2018-02-28T13:40:27.611Z","comments":true,"path":"tags/index1.html","permalink":"http://yoursite.com/tags/index1.html","excerpt":"","text":""},{"title":"","date":"2018-02-28T13:40:14.186Z","updated":"2018-02-28T13:40:14.186Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"UDP介绍","slug":"UDP介绍","date":"2017-04-19T16:00:00.000Z","updated":"2018-03-18T13:02:29.469Z","comments":true,"path":"2017/04/20/UDP介绍/","link":"","permalink":"http://yoursite.com/2017/04/20/UDP介绍/","excerpt":"","text":"UDP — 用户数据报协议 是一个无连接的简单的面向数据报的传输层协议。UDP不提供可靠性，它只是把应用程序传给IP层的数据报发送出去，但是并不能保证它们能到达目的地。由于UDP在传输数据报前不用在客户和服务器之间建立一个连接，且没有超时重发等机制，故而传输速度很快。 UDP是一种面向无连接的协议，每个数据报都是一个独立的信息，包括完整的源地址或目的地址，它在网络上以任何可能的路径传往目的地，因此能否到达目的地，到达目的地的时间以及内容的正确性都是不能被保证的。 UDP特点：UDP是面向无连接的通讯协议，UDP数据包括目的端口号和源端口号信息，由于通讯不需要连接，所以可以实现广播发送。 UDP传输数据时有大小限制，每个被传输的数据报必须限定在64KB之内。 UDP是一个不可靠的协议，发送方所发送的数据报并不一定以相同的次序到达接收方。 【适用情况】 UDP是面向消息的协议，通信时不需要建立连接，数据的传输自然是不可靠的，UDP一般用于多点通信和实时的数据业务，比如： 语音广播 视频 QQ TFTP(简单文件传送） SNMP（简单网络管理协议） RIP（路由信息协议，如报告股票市场，航空信息） DNS(域名解释） 注重速度流畅 UDP操作简单，而且仅需要较少的监护，因此通常用于局域网高可靠性的分散系统中client/server应用程序。例如视频会议系统，并不要求音频视频数据绝对的正确，只要保证连贯性就可以了，这种情况下显然使用UDP会更合理一些。 udp网络程序-发送数据 创建一个udp客户端程序的流程是简单，具体步骤如下： 创建客户端套接字 发送/接收数据 关闭套接字 代码如下： #coding=utf-8 from socket import * #1. 创建套接字 udpSocket = socket(AF_INET, SOCK_DGRAM) #2. 准备接收方的地址 sendAddr = (&apos;192.168.1.103&apos;, 8080) #3. 从键盘获取数据 sendData = raw_input(&quot;请输入要发送的数据:&quot;) #4. 发送数据到指定的电脑上 udpSocket.sendto(sendData, sendAddr) #5. 关闭套接字 udpSocket.close() 运行现象： 在Ubuntu中运行脚本： 在windows中运行“网络调试助手”： udp网络程序-发送、接收数据 创建udp网络程序-接收数据 #coding=utf-8 from socket import * #1. 创建套接字 udpSocket = socket(AF_INET, SOCK_DGRAM) #2. 准备接收方的地址 sendAddr = (&apos;192.168.1.103&apos;, 8080) #3. 从键盘获取数据 sendData = raw_input(&quot;请输入要发送的数据:&quot;) #4. 发送数据到指定的电脑上 udpSocket.sendto(sendData, sendAddr) #5. 等待接收对方发送的数据 recvData = udpSocket.recvfrom(1024) # 1024表示本次接收的最大字节数 #6. 显示对方发送的数据 print(recvData) #7. 关闭套接字 udpSocket.close() python脚本： 网络调试助手截图： udp网络程序-端口问题 会变的端口号 重新运行多次脚本，然后在“网络调试助手”中，看到的现象如下： 说明： 每重新运行一次网络程序，上图中红圈中的数字，不一样的原因在于，这个数字标识这个网络程序，当重新运行时，如果没有确定到底用哪个，系统默认会随机分配。 记住一点：这个网络程序在运行的过程中，这个就唯一标识这个程序，所以如果其他电脑上的网络程序如果想要向此程序发送数据，那么就需要向这个数字（即端口）标识的程序发送即可。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"udp","slug":"udp","permalink":"http://yoursite.com/tags/udp/"},{"name":"网络","slug":"网络","permalink":"http://yoursite.com/tags/网络/"},{"name":"通信","slug":"通信","permalink":"http://yoursite.com/tags/通信/"},{"name":"协议","slug":"协议","permalink":"http://yoursite.com/tags/协议/"}]},{"title":"socket简介","slug":"socket简介","date":"2017-04-18T16:00:00.000Z","updated":"2018-03-18T13:03:01.693Z","comments":true,"path":"2017/04/19/socket简介/","link":"","permalink":"http://yoursite.com/2017/04/19/socket简介/","excerpt":"","text":"1.本地的进程间通信（IPC）有很多种方式，例如: 队列 同步（互斥锁、条件变量等） 以上通信方式都是在一台机器上不同进程之间的通信方式，那么问题来了 网络中进程之间如何通信？ 网络中进程之间如何通信 首要解决的问题是如何唯一标识一个进程，否则通信无从谈起！ 在本地可以通过进程PID来唯一标识一个进程，但是在网络中这是行不通的。 其实TCP/IP协议族已经帮我们解决了这个问题，网络层的“ip地址”可以唯一标识网络中的主机，而传输层的“协议+端口”可以唯一标识主机中的应用程序（进程）。 这样利用ip地址，协议，端口就可以标识网络的进程了，网络中的进程通信就可以利用这个标志与其它进程进行交互。 什么是socket socket(简称 套接字) 是进程间通信的一种方式，它与其他进程间通信的一个主要不同是： 它能实现不同主机间的进程间通信，我们网络上各种各样的服务大多都是基于 Socket 来完成通信的,例如我们每天浏览网页、QQ 聊天、收发 email 等等… 创建socket 在 Python 中 使用socket 模块的函数 socket 就可以完成： socket.socket(AddressFamily, Type) 说明： 函数 socket.socket 创建一个 socket，返回该 socket 的描述符，该函数带有两个参数： Address Family：可以选择 AF_INET（用于 Internet 进程间通信） 或者 AF_UNIX（用于同一台机器进程间通信）,实际工作中常用AF_INET Type：套接字类型，可以是 SOCK_STREAM（流式套接字，主要用于 TCP 协议）或者 SOCK_DGRAM（数据报套接字，主要用于 UDP 协议） 创建一个tcp socket（tcp套接字） import socket s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) print &apos;Socket Created&apos; 创建一个udp socket（udp套接字） import socket s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) print &apos;Socket Created&apos;","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://yoursite.com/tags/网络/"},{"name":"通信","slug":"通信","permalink":"http://yoursite.com/tags/通信/"},{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"}]},{"title":"自己动手开发网络服务器（三）","slug":"自己动手开发网络服务器（三）","date":"2017-04-17T16:00:00.000Z","updated":"2018-03-15T15:01:08.509Z","comments":true,"path":"2017/04/18/自己动手开发网络服务器（三）/","link":"","permalink":"http://yoursite.com/2017/04/18/自己动手开发网络服务器（三）/","excerpt":"","text":"在第二部分中，你开发了一个能够处理HTTPGET请求的简易WSGI服务器。在上一篇的最后，我问了你一个问题：“怎样让服务器一次处理多个请求？”读完本文，你就能够完美地回答这个问题。接下来，请你做好准备，因为本文的内容非常多，节奏也很快。文中的所有代码都可以在Github仓库下载。 首先，我们简单回忆一下简易网络服务器是如何实现的，服务器要处理客户端的请求需要哪些条件。你在前面两部分文章中开发的服务器，是一个迭代式服务器（iterative server），还只能一次处理一个客户端请求。只有在处理完当前客户端请求之后，它才能接收新的客户端连接。这样，有些客户端就必须要等待自己的请求被处理了，而对于流量大的服务器来说，等待的时间就会特别长。 下面是迭代式服务器webserver3a.py的代码： ##################################################################### # Iterative server - webserver3a.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # ##################################################################### import socket SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 REQUEST_QUEUE_SIZE = 5 def handle_request(client_connection): request = client_connection.recv(1024) print(request.decode()) http_response = b&quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) def serve_forever(): listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind(SERVER_ADDRESS) listen_socket.listen(REQUEST_QUEUE_SIZE) print(&apos;Serving HTTP on port {port} ...&apos;.format(port=PORT)) while True: client_connection, client_address = listen_socket.accept() handle_request(client_connection) client_connection.close() if __name__ == &apos;__main__&apos;: serve_forever() 如果想确认这个服务器每次只能处理一个客户端的请求，我们对上述代码作简单修改，在向客户端返回响应之后，增加60秒的延迟处理时间。这个修改只有一行代码，即告诉服务器在返回响应之后睡眠60秒。 下面就是修改之后的服务器代码： ######################################################################### # Iterative server - webserver3b.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # # # # - Server sleeps for 60 seconds after sending a response to a client # ######################################################################### import socket import time SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 REQUEST_QUEUE_SIZE = 5 def handle_request(client_connection): request = client_connection.recv(1024) print(request.decode()) http_response = b&quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) time.sleep(60) # sleep and block the process for 60 seconds def serve_forever(): listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind(SERVER_ADDRESS) listen_socket.listen(REQUEST_QUEUE_SIZE) print(&apos;Serving HTTP on port {port} ...&apos;.format(port=PORT)) while True: client_connection, client_address = listen_socket.accept() handle_request(client_connection) client_connection.close() if __name__ == &apos;__main__&apos;: serve_forever() 接下来，我们启动服务器： $ python webserver3b.py 现在，我们打开一个新的终端窗口，并运行curl命令。你会立刻看到屏幕上打印出了“Hello, World!”这句话： $ curl http://localhost:8888/hello Hello, World! 接着我们立刻再打开一个终端窗口，并运行curl命令： $ curl http://localhost:8888/hello 如果你在60秒了完成了上面的操作，那么第二个curl命令应该不会立刻产生任何输出结果，而是处于挂死（hang）状态。服务器也不会在标准输出中打印这个新请求的正文。下面这张图就是我在自己的Mac上操作时的结果（右下角那个边缘高亮为黄色的窗口，显示的就是第二个curl命令挂死）： 当然，你等了足够长时间之后（超过60秒），你会看到第一个curl命令结束，然后第二个curl命令会在屏幕上打印出“Hello, World!”，之后再挂死60秒，最后才结束： 这背后的实现方式是，服务器处理完第一个curl客户端请求后睡眠60秒，才开始处理第二个请求。这些步骤是线性执行的，或者说迭代式一步一步执行的。在我们这个实例中，则是一次一个请求这样处理。 接下来，我们简单谈谈客户端与服务器之间的通信。为了让两个程序通过网络进行通信，二者均必须使用套接字。你在前两章中也看到过套接字，但到底什么是套接字？ 套接字是通信端点（communication endpoint）的抽象形式，可以让一个程序通过文件描述符（file descriptor）与另一个程序进行通信。在本文中，我只讨论Linux/Mac OS X平台上的TCP/IP套接字。其中，尤为重要的一个概念就是TCP套接字对（socket pair）。 TCP连接所使用的套接字对是一个4元组（4-tuple），包括本地IP地址、本地端口、外部IP地址和外部端口。一个网络中的每一个TCP连接，都拥有独特的套接字对。IP地址和端口号通常被称为一个套接字，二者一起标识了一个网络端点。 因此，{10.10.10.2:49152, 12.12.12.3:8888}元组组成了一个套接字对，代表客户端侧TCP连接的两个唯一端点，{12.12.12.3:8888, 10.10.10.2:49152}元组组成另一个套接字对，代表服务器侧TCP连接的两个同样端点。构成TCP连接中服务器端点的两个值分别是IP地址12.12.12.3和端口号8888，它们在这里被称为一个套接字（同理，客户端端点的两个值也是一个套接字）。 服务器创建套接字并开始接受客户端连接的标准流程如下： 1,服务器创建一个TCP/IP套接字。通过下面的Python语句实现： listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 2,服务器可以设置部分套接字选项（这是可选项，但你会发现上面那行服务器代码就可以确保你重启服务器之后，服务器会继续使用相同的地址）。 listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) 3,然后，服务器绑定地址。绑定函数为套接字指定一个本地协议地址。调用绑定函数时，你可以单独指定端口号或IP地址，也可以同时指定两个参数，甚至不提供任何参数也没问题。 listen_socket.bind(SERVER_ADDRESS) 4,接着，服务器将该套接字变成一个侦听套接字： listen_socket.listen(REQUEST_QUEUE_SIZE) listen方法只能由服务器调用，执行后会告知服务器应该接收针对该套接字的连接请求。 完成上面四步之后，服务器会开启一个循环，开始接收客户端连接，不过一次只接收一个连接。当有连接请求时，accept方法会返回已连接的客户端套接字。然后，服务器从客户端套接字读取请求数据，在标准输出中打印数据，并向客户端返回消息。最后，服务器会关闭当前的客户端连接，这时服务器又可以接收新的客户端连接了。 要通过TCP/IP协议与服务器进行通信，客户端需要作如下操作： 下面这段示例代码，实现了客户端连接至服务器，发送请求，并打印响应内容的过程： import socket # create a socket and connect to a server sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect((&apos;localhost&apos;, 8888)) # send and receive some data sock.sendall(b&apos;test&apos;) data = sock.recv(1024) print(data.decode()) 在创建套接字之后，客户端需要与服务器进行连接，这可以通过调用connect方法实现： sock.connect((&apos;localhost&apos;, 8888)) 客户端只需要提供远程IP地址或主机名，以及服务器的远程连接端口号即可。 你可能已经注意到，客户端不会调用bind和accept方法。不需要调用bind方法，是因为客户端不关心本地IP地址和本地端口号。客户端调用connect方法时，系统内核中的TCP/IP栈会自动指定本地IP地址和本地端口。本地端口也被称为临时端口（ephemeral port）。 服务器端有部分端口用于连接熟知的服务，这种端口被叫做“熟知端口”（well-known port），例如，80用于HTTP传输服务，22用于SSH协议传输。接下来，我们打开Python shell，向在本地运行的服务器发起一个客户端连接，然后查看系统内核为你创建的客户端套接字指定了哪个临时端口（在进行下面的操作之前，请先运行webserver3a.py或webserver3b.py文件，启动服务器）： &gt;&gt;&gt; import socket &gt;&gt;&gt; sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) &gt;&gt;&gt; sock.connect((&apos;localhost&apos;, 8888)) &gt;&gt;&gt; host, port = sock.getsockname()[:2] &gt;&gt;&gt; host, port (&apos;127.0.0.1&apos;, 60589) 在上面的示例中，我们看到内核为套接字指定的临时端口是60589。 在开始回答第二部分最后提的问题之前，我需要快速介绍一些其他的重要概念。稍后你就会明白我为什么要这样做。我要介绍的重要概念就是进程（process）和文件描述符（file descriptor）。 什么是进程？进程就是正在执行的程序的一个实例。举个例子，当服务器代码执行的时候，这些代码就被加载至内存中，而这个正在被执行的服务器的实例就叫做进程。系统内核会记录下有关进程的信息——包括进程ID，以便进行管理。所以，当你运行迭代式服务器webserver3a.py或webserver3b.py时，你也就开启了一个进程。 我们在终端启动webserver3a.py服务器： $ python webserver3b.py 然后，我们在另一个终端窗口中，使用ps命令来获取上面那个服务器进程的信息： $ ps | grep webserver3b | grep -v grep 7182 ttys003 0:00.04 python webserver3b.py 从ps命令的结果，我们可以看出你的确只运行了一个Python进程webserver3b。进程创建的时候，内核会给它指定一个进程ID——PID。在UNIX系统下，每个用户进程都会有一个父进程（parent process），而这个父进程也有自己的进程ID，叫做父进程ID，简称PPID。在本文中，我默认大家使用的是BASH，因此当你启动服务器的时候，系统会创建服务器进程，指定一个PID，而服务器进程的父进程PID则是BASH shell进程的PID。 接下来请自己尝试操作一下。再次打开你的Python shell程序，这会创建一个新进程，然后我们通过os.gepid()和os.getppid()这两个方法，分别获得Python shell进程的PID及它的父进程PID（即BASH shell程序的PID）。接着，我们打开另一个终端窗口，运行ps命令，grep检索刚才所得到的PPID（父进程ID，本操作时的结果是3148）。在下面的截图中，你可以看到我在Mac OS X上的操作结果： 另一个需要掌握的重要概念就是文件描述符（file descriptor）。那么，到底什么是文件描述符？文件描述符指的就是当系统打开一个现有文件、创建一个新文件或是创建一个新的套接字之后，返回给进程的那个正整型数。系统内核通过文件描述符来追踪一个进程所打开的文件。当你需要读写文件时，你也通过文件描述符说明。Python语言中提供了用于处理文件（和套接字）的高层级对象，所以你不必直接使用文件描述符来指定文件，但是从底层实现来看，UNIX系统中就是通过它们的文件描述符来确定文件和套接字的。 一般来说，UNIX shell会将文件描述符0指定给进程的标准输出，文件描述富1指定给进程的标准输出，文件描述符2指定给标准错误。 正如我前面提到的那样，即使Python语言提供了高层及的文件或类文件对象，你仍然可以对文件对象使用fileno()方法，来获取该文件相应的文件描述符。我们回到Python shell中来试验一下。 &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.stdin &lt;open file &apos;&lt;stdin&gt;&apos;, mode &apos;r&apos; at 0x102beb0c0&gt; &gt;&gt;&gt; sys.stdin.fileno() 0 &gt;&gt;&gt; sys.stdout.fileno() 1 &gt;&gt;&gt; sys.stderr.fileno() 2 在Python语言中处理文件和套接字时，你通常只需要使用高层及的文件/套接字对象即可，但是有些时候你也可能需要直接使用文件描述符。下面这个示例演示了你如何通过write()方法向标准输出中写入一个字符串，而这个write方法就接受文件描述符作为自己的参数： &gt;&gt;&gt; import sys &gt;&gt;&gt; import os &gt;&gt;&gt; res = os.write(sys.stdout.fileno(), &apos;hello\\n&apos;) hello 还有一点挺有意思——如果你知道Unix系统下一切都是文件，那么你就不会觉得奇怪了。当你在Python中创建一个套接字后，你获得的是一个套接字对象，而不是一个正整型数，但是你还是可以和上面演示的一样，通过fileno()方法直接访问这个套接字的文件描述符。 &gt;&gt;&gt; import socket &gt;&gt;&gt; sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) &gt;&gt;&gt; sock.fileno() 3 我还想再说一点：不知道大家有没有注意到，在迭代式服务器webserver3b.py的第二个示例中，我们的服务器在处理完请求后睡眠60秒，但是在睡眠期间，我们仍然可以通过curl命令与服务器建立连接？当然，curl命令并没有立刻输出结果，只是出于挂死状态，但是为什么服务器既然没有接受新的连接，客户端也没有立刻被拒绝，而是仍然继续连接至服务器呢？这个问题的答案在于套接字对象的listen方法，以及它使用的BACKLOG参数。在示例代码中，这个参数的值被我设置为REQUEST_QUEQUE_SIZE。BACKLOG参数决定了内核中外部连接请求的队列大小。当webserver3b.py服务器睡眠时，你运行的第二个curl命令之所以能够连接服务器，是因为连接请求队列仍有足够的位置。 虽然提高BACKLOG参数的值并不会让你的服务器一次处理多个客户端请求，但是业务繁忙的服务器也应该设置一个较大的BACKLOG参数值，这样accept函数就可以直接从队列中获取新连接，立刻开始处理客户端请求，而不是还要花时间等待连接建立。 呜呼！到目前为止，已经给大家介绍了很多知识。我们现在快速回顾一下之前的内容。 迭代式服务器 服务器套接字创建流程（socket, bind, listen, accept） 客户端套接字创建流程（socket, connect） 套接字对（Socket pair） 套接字 临时端口（Ephemeral port）与熟知端口（well-known port） 进程 进程ID（PID），父进程ID（PPID）以及父子关系 文件描述符（File descriptors） 套接字对象的listen方法中BACKLOG参数的意义 现在，我可以开始回答第二部分留下的问题了：如何让服务器一次处理多个请求？换句话说，如何开发一个并发服务器？ 在Unix系统中开发一个并发服务器的最简单方法，就是调用系统函数fork()。 下面就是崭新的webserver3c.py并发服务器，能够同时处理多个客户端请求： ########################################################################### # Concurrent server - webserver3c.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # # # # - Child process sleeps for 60 seconds after handling a client&apos;s request # # - Parent and child processes close duplicate descriptors # # # ########################################################################### import os import socket import time SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 REQUEST_QUEUE_SIZE = 5 def handle_request(client_connection): request = client_connection.recv(1024) print( &apos;Child PID: {pid}. Parent PID {ppid}&apos;.format( pid=os.getpid(), ppid=os.getppid(), ) ) print(request.decode()) http_response = b&quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) time.sleep(60) def serve_forever(): listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind(SERVER_ADDRESS) listen_socket.listen(REQUEST_QUEUE_SIZE) print(&apos;Serving HTTP on port {port} ...&apos;.format(port=PORT)) print(&apos;Parent PID (PPID): {pid}\\n&apos;.format(pid=os.getpid())) while True: client_connection, client_address = listen_socket.accept() pid = os.fork() if pid == 0: # child listen_socket.close() # close child copy handle_request(client_connection) client_connection.close() os._exit(0) # child exits here else: # parent client_connection.close() # close parent copy and loop over if __name__ == &apos;__main__&apos;: serve_forever() 在讨论fork的工作原理之前，请测试一下上面的代码，亲自确认一下服务器是否能够同时处理多个客户端请求。我们通过命令行启动上面这个服务器： $ python webserver3c.py 然后输入之前迭代式服务器示例中的两个curl命令。现在，即使服务器子进程在处理完一个客户端请求之后会睡眠60秒，但是并不会影响其他客户端，因为它们由不同的、完全独立的进程处理。你应该可以立刻看见curl命令输出“Hello, World”，然后挂死60秒。你可以继续运行更多的curl命令，所有的命令都会输出服务器的响应结果——“Hello, World”，不会有任何延迟。你可以试试。 关于fork()函数有一点最为重要，就是你调用fork一次，但是函数却会返回两次：一次是在父进程里返回，另一次是在子进程中返回。当你fork一个进程时，返回给子进程的PID是0，而fork返回给父进程的则是子进程的PID。 我还记得，第一次接触并使用fork函数时，自己感到非常不可思议。我觉得这就好像一个魔法。之前还是一个线性的代码，突然一下子克隆了自己，出现了并行运行的相同代码的两个实例。我当时真的觉得这和魔法也差不多了。 当父进程fork一个新的子进程时，子进程会得到父进程文件描述符的副本： 你可能也注意到了，上面代码中的父进程关闭了客户端连接： else: # parent client_connection.close() # close parent copy and loop over 那为什么父进程关闭了套接字之后，子进程却仍然能够从客户端套接字中读取数据呢？答案就在上面的图片里。系统内核根据文件描述符计数（descriptor reference counts）来决定是否关闭套接字。系统只有在描述符计数变为0时，才会关闭套接字。当你的服务器创建一个子进程时，子进程就会获得父进程文件描述符的副本，系统内核则会增加这些文件描述符的计数。在一个父进程和一个子进程的情况下，客户端套接字的文件描述符计数为2。当上面代码中的父进程关闭客户端连接套接字时，只是让套接字的计数减为1，还不够让系统关闭套接字。子进程同样关闭了父进程侦听套接字的副本，因为子进程不关心要不要接收新的客户端连接，只关心如何处理连接成功的客户端所发出的请求。 listen_socket.close() # close child copy 稍后，我会给大家介绍如果不关闭重复的描述符的后果。 从上面并行服务器的源代码可以看出，服务器父进程现在唯一的作用，就是接受客户端连接，fork一个新的子进程来处理该客户端连接，然后回到循环的起点，准备接受其他的客户端连接，仅此而已。服务器父进程并不会处理客户端请求，而是由它的子进程来处理。 谈得稍远一点。我们说两个事件是并行时，到底是什么意思？ 我们说两个事件是并行的，通常指的是二者同时发生。这是简单的定义，但是你应该牢记它的严格定义： 如果你不能分辨出哪个程序会先执行，那么二者就是并行的。 现在又到了回顾目前已经介绍的主要观点和概念。 Unix系统中开发并行服务器最简单的方法，就是调用fork()函数 当一个进程fork新进程时，它就成了新创建进程的父进程 在调用fork之后，父进程和子进程共用相同的文件描述符 系统内核通过描述符计数来决定是否关闭文件/套接字 服务器父进程的角色：它现在所做的只是接收来自客户端的新连接，fork一个子进程来处理该客户端的请求，然后回到循环的起点，准备接受新的客户端连接 接下来，我们看看如果不关闭父进程和子进程中的重复套接字描述符，会发生什么情况。下面的并行服务器（webserver3d.py）作了一些修改，确保服务器不关闭重复的： ########################################################################### # Concurrent server - webserver3d.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # ########################################################################### import os import socket SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 REQUEST_QUEUE_SIZE = 5 def handle_request(client_connection): request = client_connection.recv(1024) http_response = b&quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) def serve_forever(): listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind(SERVER_ADDRESS) listen_socket.listen(REQUEST_QUEUE_SIZE) print(&apos;Serving HTTP on port {port} ...&apos;.format(port=PORT)) clients = [] while True: client_connection, client_address = listen_socket.accept() # store the reference otherwise it&apos;s garbage collected # on the next loop run clients.append(client_connection) pid = os.fork() if pid == 0: # child listen_socket.close() # close child copy handle_request(client_connection) client_connection.close() os._exit(0) # child exits here else: # parent # client_connection.close() print(len(clients)) if __name__ == &apos;__main__&apos;: serve_forever() 启动服务器： $ python webserver3d.py 然后通过curl命令连接至服务器： $ curl http://localhost:8888/hello Hello, World! 我们看到，curl命令打印了并行服务器的响应内容，但是并没有结束，而是继续挂死。服务器出现了什么不同情况吗？服务器不再继续睡眠60秒：它的子进程会积极处理客户端请求，处理完成后就关闭客户端连接，然后结束运行，但是客户端的curl命令却不会终止。 那么为什么curl命令会没有结束运行呢？原因在于重复的文件描述符（duplicate file descriptor）。当子进程关闭客户端连接时，系统内核会减少客户端套接字的计数，变成了1。服务器子进程结束了，但是客户端套接字并没有关闭，因为那个套接字的描述符计数并没有变成0，导致系统没有向客户端发送终止包（termination packet，用TCP/IP的术语来说叫做FIN），也就是说客户端仍然在线。但是还有另一个问题。如果你一直运行的服务器不去关闭重复的文件描述符，服务器最终就会耗光可用的文件服务器： 按下Control-C，关闭webserver3d.py服务器，然后通过shell自带的ulimit命令查看服务器进程可以使用的默认资源： $ ulimit -a core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited scheduling priority (-e) 0 file size (blocks, -f) unlimited pending signals (-i) 3842 max locked memory (kbytes, -l) 64 max memory size (kbytes, -m) unlimited open files (-n) 1024 pipe size (512 bytes, -p) 8 POSIX message queues (bytes, -q) 819200 real-time priority (-r) 0 stack size (kbytes, -s) 8192 cpu time (seconds, -t) unlimited max user processes (-u) 3842 virtual memory (kbytes, -v) unlimited file locks (-x) unlimited 从上面的结果中，我们可以看到：在我这台Ubuntu电脑上，服务器进程可以使用的文件描述符（打开的文件）最大数量为1024。 现在，我们来看看如果服务器不关闭重复的文件描述符，服务器会不会耗尽可用的文件描述符。我们在现有的或新开的终端窗口里，将服务器可以使用的最大文件描述符数量设置为256： $ ulimit -n 256 在刚刚运行了$ ulimit -n 256命令的终端里，我们开启webserver3d.py服务器： $ python webserver3d.py 然后通过下面的client3.py客户端来测试服务器。 $ python webserver3d.py 然后通过下面的client3.py客户端来测试服务器。 ##################################################################### # Test client - client3.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # ##################################################################### import argparse import errno import os import socket SERVER_ADDRESS = &apos;localhost&apos;, 8888 REQUEST = b&quot;&quot;&quot;\\ GET /hello HTTP/1.1 Host: localhost:8888 &quot;&quot;&quot; def main(max_clients, max_conns): socks = [] for client_num in range(max_clients): pid = os.fork() if pid == 0: for connection_num in range(max_conns): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect(SERVER_ADDRESS) sock.sendall(REQUEST) socks.append(sock) print(connection_num) os._exit(0) if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser( description=&apos;Test client for LSBAWS.&apos;, formatter_class=argparse.ArgumentDefaultsHelpFormatter, ) parser.add_argument( &apos;--max-conns&apos;, type=int, default=1024, help=&apos;Maximum number of connections per client.&apos; ) parser.add_argument( &apos;--max-clients&apos;, type=int, default=1, help=&apos;Maximum number of clients.&apos; ) args = parser.parse_args() main(args.max_clients, args.max_conns) 打开一个新终端窗口，运行client3.py，并让客户端创建300个与服务器的并行连接： $ python client3.py --max-clients=300 很快你的服务器就会崩溃。下面是我的虚拟机上抛出的异常情况： 问题很明显——服务器应该关闭重复的描述符。但即使你关闭了这些重复的描述符，你还没有彻底解决问题，因为你的服务器还存在另一个问题，那就是僵尸进程！ 没错，你的服务器代码确实会产生僵尸进程。我们来看看这是怎么回事。再次运行服务器： $ python webserver3d.py 在另一个终端窗口中运行下面的curl命令： $ curl http://localhost:8888/hello 现在，我们运行ps命令，看看都有哪些正在运行的Python进程。下面是我的Ubuntu虚拟机中的结果： $ ps auxw | grep -i python | grep -v grep vagrant 9099 0.0 1.2 31804 6256 pts/0 S+ 16:33 0:00 python webserver3d.py vagrant 9102 0.0 0.0 0 0 pts/0 Z+ 16:33 0:00 [python] &lt;defunct&gt; 我们发现，第二行中显示的这个进程的PID为9102，状态是Z+，而进程的名称叫做。这就是我们要找的僵尸进程。僵尸进程的问题在于你无法杀死它们。 即使你试图通过$ kill -9命令杀死僵尸进程，它们还是会存活下来。你可以试试看。 到底什么是僵尸进程，服务器又为什么会创建这些进程？僵尸进程其实是已经结束了的进程，但是它的父进程并没有等待进程结束，所以没有接收到进程结束的状态信息。当子进程在父进程之前退出，系统就会将子进程变成一个僵尸进程，保留原子进程的部分信息，方便父进程之后获取。系统所保留的信息通常包括进程ID、进程结束状态和进程的资源使用情况。好吧，这样说僵尸进程也有自己存在的理由，但是如果服务器不处理好这些僵尸进程，系统就会堵塞。我们来看看是否如此。首先，停止正在运行的服务器，然后在新终端窗口中，使用ulimit命令将最大用户进程设置为400（还要确保将打开文件数量限制设置到一个较高的值，这里我们设置为500）。 $ ulimit -u 400 $ ulimit -n 500 然后在同一个窗口中启动webserver3d.py服务器： $ ulimit -u 400 command: $ python webserver3d.py 在新终端窗口中，启动客户端client3.py，让客户端创建500个服务器并行连接： $ python client3.py --max-clients=500 结果，我们发现很快服务器就因为OSError而崩溃：这个异常指的是暂时没有足够的资源。服务器试图创建新的子进程时，由于已经达到了系统所允许的最大可创建子进程数，所以抛出这个异常。下面是我的虚拟机上的报错截图。 你也看到了，如果长期运行的服务器不处理好僵尸进程，将会出现重大问题。稍后我会介绍如何处理僵尸进程。 我们先回顾一下目前已经学习的知识点： 如果你不关闭重复的文件描述符，由于客户端连接没有中断，客户端程序就不会结束。 如果你不关闭重复的文件描述符，你的服务器最终会消耗完可用的文件描述符（最大打开文件数） 当你fork一个子进程后，如果子进程在父进程之前退出，而父进程又没有等待进程，并获取它的结束状态，那么子进程就会变成僵尸进程。 僵尸进程也需要消耗资源，也就是内存。如果不处理好僵尸进程，你的服务器最终会消耗完可用的进程数（最大用户进程数）。 你无法杀死僵尸进程，你需要等待子进程结束。 那么，你要怎么做才能处理掉僵尸进程呢？你需要修改服务器代码，等待僵尸进程返回其结束状态（termination status）。要实现这点，你只需要在代码中调用wait系统函数即可。不过，这种方法并不是最理想的方案，因为如果你调用wait后，却没有结束了的子进程，那么wait调用将会阻塞服务器，相当于阻止了服务器处理新的客户端请求。那么还有其他的办法吗？答案是肯定的，其中一种办法就是将wait函数调用与信号处理函数（signal handler）结合使用。 这种方法的具体原理如下。当子进程退出时，系统内核会发送一个SIGCHLD信号。父进程可以设置一个信号处理函数，用于异步监测SIGCHLD事件，然后再调用wait，等待子进程结束并获取其结束状态，这样就可以避免产生僵尸进程。 ########################################################################### # Concurrent server - webserver3e.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # ########################################################################### import os import signal import socket import time SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 REQUEST_QUEUE_SIZE = 5 def grim_reaper(signum, frame): pid, status = os.wait() print( &apos;Child {pid} terminated with status {status}&apos; &apos;\\n&apos;.format(pid=pid, status=status) ) def handle_request(client_connection): request = client_connection.recv(1024) print(request.decode()) http_response = b&quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) # sleep to allow the parent to loop over to &apos;accept&apos; and block there time.sleep(3) def serve_forever(): listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind(SERVER_ADDRESS) listen_socket.listen(REQUEST_QUEUE_SIZE) print(&apos;Serving HTTP on port {port} ...&apos;.format(port=PORT)) signal.signal(signal.SIGCHLD, grim_reaper) while True: client_connection, client_address = listen_socket.accept() pid = os.fork() if pid == 0: # child listen_socket.close() # close child copy handle_request(client_connection) client_connection.close() os._exit(0) else: # parent client_connection.close() if __name__ == &apos;__main__&apos;: serve_forever() 启动服务器： $ python webserver3e.py 再次使用curl命令，向修改后的并发服务器发送一个请求： $ curl http://localhost:8888/hello 我们来看服务器的反应： 发生了什么事？accept函数调用报错了。 子进程退出时，父进程被阻塞在accept函数调用的地方，但是子进程的退出导致了SIGCHLD事件，这也激活了信号处理函数。信号函数执行完毕之后，就导致了accept系统函数调用被中断: 别担心，这是个非常容易解决的问题。你只需要重新调用accept即可。下面我们再修改一下服务器代码（webserver3f.py），就可以解决这个问题： ########################################################################### # Concurrent server - webserver3f.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # ########################################################################### import errno import os import signal import socket SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 REQUEST_QUEUE_SIZE = 1024 def grim_reaper(signum, frame): pid, status = os.wait() def handle_request(client_connection): request = client_connection.recv(1024) print(request.decode()) http_response = b&quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) def serve_forever(): listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind(SERVER_ADDRESS) listen_socket.listen(REQUEST_QUEUE_SIZE) print(&apos;Serving HTTP on port {port} ...&apos;.format(port=PORT)) signal.signal(signal.SIGCHLD, grim_reaper) while True: try: client_connection, client_address = listen_socket.accept() except IOError as e: code, msg = e.args # restart &apos;accept&apos; if it was interrupted if code == errno.EINTR: continue else: raise pid = os.fork() if pid == 0: # child listen_socket.close() # close child copy handle_request(client_connection) client_connection.close() os._exit(0) else: # parent client_connection.close() # close parent copy and loop over if __name__ == &apos;__main__&apos;: serve_forever() 启动修改后的服务器： $ python webserver3f.py 通过curl命令向服务器发送一个请求： $ curl http://localhost:8888/hello 看到了吗？没有再报错了。现在，我们来确认下服务器没有再产生僵尸进程。只需要运行ps命令，你就会发现没有Python进程的状态是Z+了。太棒了！没有僵尸进程捣乱真是太好了。 如果你fork一个子进程，却不等待进程结束，该进程就会变成僵尸进程。 使用SIGCHLD时间处理函数来异步等待进程结束，获取其结束状态。 使用事件处理函数时，你需要牢记系统函数调用可能会被中断，要做好这类情况发生得准备。 好了，目前一切正常。没有其他问题了，对吗？呃，基本上是了。再次运行webserver3f.py，然后通过client3.py创建128个并行连接： $ python client3.py --max-clients 128 现在再次运行ps命令： $ ps auxw | grep -i python | grep -v grep 噢，糟糕！僵尸进程又出现了！ 这次又是哪里出了问题？当你运行128个并行客户端，建立128个连接时，服务器的子进程处理完请求，几乎是同一时间退出的，这就触发了一大波的SIGCHLD信号发送至父进程。但问题是这些信号并没有进入队列，所以有几个信号漏网，没有被服务器处理，这就导致出现了几个僵尸进程。 这个问题的解决方法，就是在SIGCHLD事件处理函数使用waitpid，而不是wait，再调用waitpid时增加WNOHANG选项，确保所有退出的子进程都会被处理。下面就是修改后的代码，webserver3g.py： ########################################################################### # Concurrent server - webserver3g.py # # # # Tested with Python 2.7.9 &amp; Python 3.4 on Ubuntu 14.04 &amp; Mac OS X # ########################################################################### import errno import os import signal import socket SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 REQUEST_QUEUE_SIZE = 1024 def grim_reaper(signum, frame): while True: try: pid, status = os.waitpid( -1, # Wait for any child process os.WNOHANG # Do not block and return EWOULDBLOCK error ) except OSError: return if pid == 0: # no more zombies return def handle_request(client_connection): request = client_connection.recv(1024) print(request.decode()) http_response = b&quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) def serve_forever(): listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind(SERVER_ADDRESS) listen_socket.listen(REQUEST_QUEUE_SIZE) print(&apos;Serving HTTP on port {port} ...&apos;.format(port=PORT)) signal.signal(signal.SIGCHLD, grim_reaper) while True: try: client_connection, client_address = listen_socket.accept() except IOError as e: code, msg = e.args # restart &apos;accept&apos; if it was interrupted if code == errno.EINTR: continue else: raise pid = os.fork() if pid == 0: # child listen_socket.close() # close child copy handle_request(client_connection) client_connection.close() os._exit(0) else: # parent client_connection.close() # close parent copy and loop over if __name__ == &apos;__main__&apos;: serve_forever() 启动服务器： $ python webserver3g.py 使用客户端client3.py进行测试： $ python client3.py --max-clients 128 现在请确认不会再出现僵尸进程了。 恭喜大家！现在已经自己开发了一个简易的并发服务器，这个代码可以作为你以后开发生产级别的网络服务器的基础。 最后给大家留一个练习题，把第二部分中的WSGI修改为并发服务器。最终的代码可以在这里查看。不过请你在自己实现了之后再查看。 接下来该怎么办？借用乔希·比林斯（19世纪著名幽默大师）的一句话： 要像一张邮票，坚持一件事情直到你到达目的地。 原文链接：http://ruslanspivak.com/lsbaws-part3/ 译文链接：http://codingpy.com/article/build-a-simple-web-server-part-three/","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"}]},{"title":"自己动手开发网络服务器（二）","slug":"自己动手开发网络服务器（二）","date":"2017-04-16T16:00:00.000Z","updated":"2018-03-15T14:32:47.064Z","comments":true,"path":"2017/04/17/自己动手开发网络服务器（二）/","link":"","permalink":"http://yoursite.com/2017/04/17/自己动手开发网络服务器（二）/","excerpt":"","text":"在《自己动手开发网络服务器（一）》中，我给大家留了一个问题：如何在不对服务器代码作任何修改的情况下，通过该服务器运行Djando应用、Flask应用和Pyramid应用，同时满足这些不同网络框架的要求？读完这篇文章，你就可以回答这个问题了。 以前，你选择的Python网络框架将会限制所能够使用的网络服务器，反之亦然。如果框架和服务器在设计时就是可以相互匹配的，那你就不会面临这个问题： 但是如果你试图将设计不相匹配的服务器与框架相结合，那么你肯定就会碰到下面这张图所展示的这个问题： 这就意味着，你基本上只能使用能够正常运行的服务器与框架组合，而不能选择你希望使用的服务器或框架。 那么，你怎样确保可以在不修改网络服务器代码或网络框架代码的前提下，使用自己选择的服务器，并且匹配多个不同的网络框架呢？为了解决这个问题，就出现了Python Web服务器网关接口（Python Web Server Gateway Interface，简称“WSGI”）。 WSGI的出现，让开发者可以将网络框架与网络服务器的选择分隔开来，不再相互限制。现在，你可以真正地将不同的网络服务器与网络开发框架进行混合搭配，选择满足自己需求的组合。例如，你可以使用Gunicorn或Nginx/uWSGI或Waitress服务器来运行Django、Flask或Pyramid应用。正是由于服务器和框架均支持WSGI，才真正得以实现二者之间的自由混合搭配。 所以，WSGI就是我在上一篇文章中所留问题的答案。你的网络服务器必须实现一个服务器端的WSGI接口，而目前所有现代Python网络框架都已经实现了框架端的WSGI接口，这样开发者不需要修改服务器的代码，就可以支持某个网络框架。 网络服务器和网络框架支持WSGI协议，不仅让应用开发者选择符合自己需求的组合，同时也有利于服务器和框架的开发者，因为他们可以将注意力集中在自己擅长的领域，而不是相互倾轧。其他编程语言也拥有类似的接口：例如Java的Servlet API和Ruby的Rack。 口说无凭，我猜你肯定在想：“无代码无真相！”既然如此，我就在这里给出一个非常简单的WSGI服务器实现： # Tested with Python 2.7.9, Linux &amp; Mac OS X import socket import StringIO import sys class WSGIServer(object): address_family = socket.AF_INET socket_type = socket.SOCK_STREAM request_queue_size = 1 def __init__(self, server_address): # Create a listening socket self.listen_socket = listen_socket = socket.socket( self.address_family, self.socket_type ) # Allow to reuse the same address listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # Bind listen_socket.bind(server_address) # Activate listen_socket.listen(self.request_queue_size) # Get server host name and port host, port = self.listen_socket.getsockname()[:2] self.server_name = socket.getfqdn(host) self.server_port = port # Return headers set by Web framework/Web application self.headers_set = [] def set_app(self, application): self.application = application def serve_forever(self): listen_socket = self.listen_socket while True: # New client connection self.client_connection, client_address = listen_socket.accept() # Handle one request and close the client connection. Then # loop over to wait for another client connection self.handle_one_request() def handle_one_request(self): self.request_data = request_data = self.client_connection.recv(1024) # Print formatted request data a la &apos;curl -v&apos; print(&apos;&apos;.join( &apos;&lt; {line}\\n&apos;.format(line=line) for line in request_data.splitlines() )) self.parse_request(request_data) # Construct environment dictionary using request data env = self.get_environ() # It&apos;s time to call our application callable and get # back a result that will become HTTP response body result = self.application(env, self.start_response) # Construct a response and send it back to the client self.finish_response(result) def parse_request(self, text): request_line = text.splitlines()[0] request_line = request_line.rstrip(&apos;\\r\\n&apos;) # Break down the request line into components (self.request_method, # GET self.path, # /hello self.request_version # HTTP/1.1 ) = request_line.split() def get_environ(self): env = {} # The following code snippet does not follow PEP8 conventions # but it&apos;s formatted the way it is for demonstration purposes # to emphasize the required variables and their values # # Required WSGI variables env[&apos;wsgi.version&apos;] = (1, 0) env[&apos;wsgi.url_scheme&apos;] = &apos;http&apos; env[&apos;wsgi.input&apos;] = StringIO.StringIO(self.request_data) env[&apos;wsgi.errors&apos;] = sys.stderr env[&apos;wsgi.multithread&apos;] = False env[&apos;wsgi.multiprocess&apos;] = False env[&apos;wsgi.run_once&apos;] = False # Required CGI variables env[&apos;REQUEST_METHOD&apos;] = self.request_method # GET env[&apos;PATH_INFO&apos;] = self.path # /hello env[&apos;SERVER_NAME&apos;] = self.server_name # localhost env[&apos;SERVER_PORT&apos;] = str(self.server_port) # 8888 return env def start_response(self, status, response_headers, exc_info=None): # Add necessary server headers server_headers = [ (&apos;Date&apos;, &apos;Tue, 31 Mar 2015 12:54:48 GMT&apos;), (&apos;Server&apos;, &apos;WSGIServer 0.2&apos;), ] self.headers_set = [status, response_headers + server_headers] # To adhere to WSGI specification the start_response must return # a &apos;write&apos; callable. We simplicity&apos;s sake we&apos;ll ignore that detail # for now. # return self.finish_response def finish_response(self, result): try: status, response_headers = self.headers_set response = &apos;HTTP/1.1 {status}\\r\\n&apos;.format(status=status) for header in response_headers: response += &apos;{0}: {1}\\r\\n&apos;.format(*header) response += &apos;\\r\\n&apos; for data in result: response += data # Print formatted response data a la &apos;curl -v&apos; print(&apos;&apos;.join( &apos;&gt; {line}\\n&apos;.format(line=line) for line in response.splitlines() )) self.client_connection.sendall(response) finally: self.client_connection.close() SERVER_ADDRESS = (HOST, PORT) = &apos;&apos;, 8888 def make_server(server_address, application): server = WSGIServer(server_address) server.set_app(application) return server if __name__ == &apos;__main__&apos;: if len(sys.argv) &lt; 2: sys.exit(&apos;Provide a WSGI application object as module:callable&apos;) app_path = sys.argv[1] module, application = app_path.split(&apos;:&apos;) module = __import__(module) application = getattr(module, application) httpd = make_server(SERVER_ADDRESS, application) print(&apos;WSGIServer: Serving HTTP on port {port} ...\\n&apos;.format(port=PORT)) httpd.serve_forever() 上面的代码比第一部分的服务器实现代码要长的多，但是这些代码实际也不算太长，只有不到150行，大家理解起来并不会太困难。上面这个服务器的功能也更多——它可以运行你使用自己喜欢的框架所写出来的网络应用，无论你选择Pyramid、Flask、Django或是其他支持WSGI协议的框架。 你不信？你可以自己测试一下，看看结果如何。将上述代码保存为webserver2.py，或者直接从我的Github仓库下载。如果你运行该文件时没有提供任何参数，那么程序就会报错并退出。 $ python webserver2.py Provide a WSGI application object as module:callable 上述程序设计的目的，就是运行你开发的网络应用，但是你还需要满足一些它的要求。要运行服务器，你只需要安装Python即可。但是要运行使用Pyramid、Flask和Django等框架开发的网络应用，你还需要先安装这些框架。我们接下来安装这三种框架。我倾向于使用virtualenv安装。请按照下面的提示创建并激活一个虚拟环境，然后安装这三个网络框架。 $ [sudo] pip install virtualenv $ mkdir ~/envs $ virtualenv ~/envs/lsbaws/ $ cd ~/envs/lsbaws/ $ ls bin include lib $ source bin/activate (lsbaws) $ pip install pyramid (lsbaws) $ pip install flask (lsbaws) $ pip install django 接下来，你需要创建一个网络应用。我们首先创建Pyramid应用。将下面的代码保存为pyramidapp.py文件，放至webserver2.py所在的文件夹中，或者直接从我的Github仓库下载该文件： from pyramid.config import Configurator from pyramid.response import Response def hello_world(request): return Response( &apos;Hello world from Pyramid!\\n&apos;, content_type=&apos;text/plain&apos;, ) config = Configurator() config.add_route(&apos;hello&apos;, &apos;/hello&apos;) config.add_view(hello_world, route_name=&apos;hello&apos;) app = config.make_wsgi_app() 现在，你可以通过自己开发的网络服务器来启动上面的Pyramid应用。 (lsbaws) $ python webserver2.py pyramidapp:app WSGIServer: Serving HTTP on port 8888 ... 在运行webserver2.py时，你告诉自己的服务器去加载pyramidapp模块中的app可调用对象（callable）。你的服务器现在可以接收HTTP请求，并将请求中转至你的Pyramid应用。应用目前只能处理一个路由（route）：/hello。在浏览器的地址栏输入http://localhost:8888/hello，按下回车键，观察会出现什么情况： 你还可以在命令行使用curl命令，来测试服务器运行情况： $ curl -v http://localhost:8888/hello ... 接下来我们创建Flask应用。重复上面的步骤。 from flask import Flask from flask import Flask from flask import Response flask_app = Flask(&apos;flaskapp&apos;) @flask_app.route(&apos;/hello&apos;) def hello_world(): return Response( &apos;Hello world from Flask!\\n&apos;, mimetype=&apos;text/plain&apos; ) app = flask_app.wsgi_app 将上面的代码保存为flaskapp.py，或者直接从我的Github仓库下载文件，并运行： (lsbaws) $ python webserver2.py flaskapp:app WSGIServer: Serving HTTP on port 8888 ... 然后在浏览器地址栏输入http://localhost:8888/hello，并按下回车： 同样，在命令行使用curl命令，看看服务器是否会返回Flask应用生成的信息： $ curl -v http://localhost:8888/hello ... 这个服务器是不是也能支持Django应用？试一试就知道了！不过接下来的操作更为复杂一些，我建议大家克隆整个仓库，并使用其中的djangoapp.py文件。下面的代码将一个名叫helloworld的Django应用添加至当前的Python路径中，然后导入了该项目的WSGI应用。 import sys sys.path.insert(0, &apos;./helloworld&apos;) from helloworld import wsgi app = wsgi.application 将上面的代码保存为djangoapp.py，并使用你开发的服务器运行这个Django应用。 (lsbaws) $ python webserver2.py djangoapp:app WSGIServer: Serving HTTP on port 8888 ... 同样，在浏览器中输入http://localhost:8888/hello，并按下回车键： 接下来，和前面几次一样，你通过命令行使用curl命令进行测试，确认了这个Djando应用成功处理了你发出的请求： $ curl -v http://localhost:8888/hello ... 你有没有按照上面的步骤测试？你做到了让服务器支持全部三种框架吗？如果没有，请尽量自己动手操作。阅读代码很重要，但这系列文章的目的在于重新开发，而这意味着你需要自己亲自动手。最好是你自己重新输入所有的代码，并确保代码运行结果符合预期。 经过上面的介绍，你应该已经认识到了WSGI的强大之处：它可以让你自由混合搭配网络服务器和框架。WSGI为Python网络服务器与Python网络框架之间的交互提供了一个极简的接口，而且非常容易在服务器端和框架端实现。下面的代码段分别展示了服务器端和框架端的WSGI接口： def run_application(application): &quot;&quot;&quot;Server code.&quot;&quot;&quot; # This is where an application/framework stores # an HTTP status and HTTP response headers for the server # to transmit to the client headers_set = [] # Environment dictionary with WSGI/CGI variables environ = {} def start_response(status, response_headers, exc_info=None): headers_set[:] = [status, response_headers] # Server invokes the ‘application&apos; callable and gets back the # response body result = application(environ, start_response) # Server builds an HTTP response and transmits it to the client … def app(environ, start_response): &quot;&quot;&quot;A barebones WSGI app.&quot;&quot;&quot; start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;, &apos;text/plain&apos;)]) return [&apos;Hello world!&apos;] run_application(app) 下面给大家解释一下上述代码的工作原理： 网络框架提供一个命名为application的可调用对象（WSGI协议并没有指定如何实现这个对象）。服务器每次从HTTP客户端接收请求之后，调用application。它会向可调用对象传递一个名叫environ的字典作为参数，其中包含了WSGI/CGI的诸多变量，以及一个名为start_response的可调用对象。框架/应用生成HTTP状态码以及HTTP响应报头（HTTP response headers），然后将二者传递至start_response，等待服务器保存。此外，框架/应用还将返回响应的正文。服务器将状态码、响应报头和响应正文组合成HTTP响应，并返回给客户端（这一步并不属于WSGI协议）。下面这张图直观地说明了WSGI接口的情况： 有一点要提醒大家，当你使用上述框架开发网络应用的时候，你处理的是更高层级的逻辑，并不会直接处理WSGI协议相关的要求，但是我很清楚，既然你正在看这篇文章，你一定对框架端的WSGI接口很感兴趣。所以，我们接下来在不使用Pyramid、Flask或Djando框架的前提下，自己开发一个极简的WSGI网络应用/网络框架，并使用WSGI服务器运行该应用： def app(environ, start_response): &quot;&quot;&quot;A barebones WSGI application. This is a starting point for your own Web framework :) &quot;&quot;&quot; status = &apos;200 OK&apos; response_headers = [(&apos;Content-Type&apos;, &apos;text/plain&apos;)] start_response(status, response_headers) return [&apos;Hello world from a simple WSGI application!\\n&apos;] 将上述代码保存为wsgiapp.py文件，或者直接从我的Github仓库下载，然后利用网络服务器运行该应用： (lsbaws) $ python webserver2.py wsgiapp:app WSGIServer: Serving HTTP on port 8888 ... 在浏览器中输入下图中的地址，然后按回车键。结果应该是这样的： 你刚刚自己编写了一个极简的WSGI网络框架！太不可思议了。 接下来，我们重新分析服务器返回给客户端的对象。下面这张图展示的是你通过HTTP客户端调用Pyramid应用后，服务器生成的HTTP响应： 上图中的响应与你在第一篇中看到的有些类似，但是也有明显不同之处。举个例子，其中就出现了你之前没有看到过的4歌HTTP报头：Content-Type，Content-Length，Date和Server。这些事网络服务器返回的响应对象通常都会包含的报头。不过，这四个都不是必须的。报头的目的是传递有关HTTP请求/响应的额外信息。 既然你已经对WSGI接口有了更深的理解，下面这张图对响应对象的内容进行了更详细的解释，说明了每条内容是如何产生的。 到目前为止，我还没有介绍过environ字典的具体内容，但简单来说，它是一个必须包含着WSGI协议所指定的某些WSGI和CGI变量。服务器从HTTP请求中获取字典所需的值。下面这张图展示的是字典的详细内容： 网络框架通过该字典提供的信息，根据指定的路由和请求方法等参数来决定使用哪个视图（views），从哪里读取请求正文，以及如何输出错误信息。 截至目前，你已经成功创建了自己的支持WSGI协议的网络服务器，还利用不同的网络框架开发了多个网络应用。另外，你还自己开发了一个极简的网络框架。本文介绍的内容不可谓不丰富。我们接下来回顾一下WSGI网络服务器如何处理HTTP请求： 首先，服务器启动并加载网络框架/应用提供的application可调用对象然后，服务器读取一个请求信息然后，服务器对请求进行解析然后，服务器使用请求数据创建一个名叫environ的字典然后，服务器以environ字典和start_response可调用对象作为参数，调用application，并获得应用生成的响应正文。然后，服务器根据调用application对象后返回的数据，以及start_response设置的状态码和响应标头，构建一个HTTP响应。最后，服务器将HTTP响应返回至客户端。 以上就是第二部分的所有内容。你现在已经拥有了一个正常运行的WSGI服务器，可以支持通过遵守WSGI协议的网络框架所写的网络应用。最棒的是，这个服务器可以不需要作任何代码修改，就可以与多个网络框架配合使用。 最后，我再给大家留一道思考题：怎样让服务器一次处理多个请求？ 原文链接：http://ruslanspivak.com/lsbaws-part2/ 译文链接：http://codingpy.com/article/build-a-simple-web-server-part-two/","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"}]},{"title":"自己动手开发网络服务器（一）","slug":"自己动手开发网络服务器（一）","date":"2017-04-13T16:00:00.000Z","updated":"2018-03-15T14:22:29.217Z","comments":true,"path":"2017/04/14/自己动手开发网络服务器（一）/","link":"","permalink":"http://yoursite.com/2017/04/14/自己动手开发网络服务器（一）/","excerpt":"","text":"————————————————————————————————————————————————— 原文链接：http://ruslanspivak.com/lsbaws-part1/ 译文链接：http://codingpy.com/article/build-a-simple-web-server-part-one/ 有一天，一位女士散步时经过一个工地，看见有三个工人在干活。她问第一个人，“你在做什么？”第一个人有点不高兴，吼道“难道你看不出来我在砌砖吗？”女士对这个答案并不满意，接着问第二个人他在做什么。第二个人回答道，“我正在建造一堵砖墙。”然后，他转向第一个人，说道：“嘿，你砌的砖已经超过墙高了。你得把最后一块砖拿下来。”女士对这个答案还是不满意，她接着问第三个人他在做什么。第三个人抬头看着天空，对她说：“我在建造这个世界上有史以来最大的教堂”。就在他望着天空出神的时候，另外两个人已经开始争吵多出的那块砖。他慢慢转向前两个人，说道：“兄弟们，别管那块砖了。这是一堵内墙,之后还会被刷上石灰的，没人会注意到这块砖。接着砌下层吧。” 这个故事的寓意在于，当你掌握了整个系统的设计，明白不同的组件是以何种方式组合在一起的（砖块，墙，教堂）时候，你就能够更快地发现并解决问题（多出的砖块）。 但是，这个故事与从头开发一个网络服务器有什么关系呢？ 在我看来，要成为一名更优秀的程序员，你必须更好地理解自己日常使用的软件系统，而这就包括了编程语言、编译器、解释器、数据库与操作系统、网络服务器和网络开发框架。而要想更好、更深刻地理解这些系统，你必须从头重新开发这些系统，一步一个脚印地重来一遍。 孔子曰：不闻不若闻之，闻之不若见之，见之不若知之，知之不若行之。 我希望你读到这里的时候，已经认可了通过重新开发不同软件系统来学习其原理这种方式。 《自己动手开发网络服务器》会分为三个部分，将介绍如何从头开发一个简易网络服务器。我们这就开始吧。 首先，到底什么是网络服务器？ 简而言之，它是在物理服务器上搭建的一个网络连接服务器（networking server），永久地等待客户端发送请求。当服务器收到请求之后，它会生成响应并将其返回至客户端。客户端与服务器之间的通信，是以HTTP协议进行的。客户端可以是浏览器，也可以是任何支持HTTP协议的软件。 那么，网络服务器的简单实现形式会是怎样的呢？下面是我对此的理解。示例代码使用Python语言实现，不过即使你不懂Python语言，你应该也可以从代码和下面的解释中理解相关的概念： import socket HOST, PORT = &apos;&apos;, 8888 listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listen_socket.bind((HOST, PORT)) listen_socket.listen(1) print &apos;Serving HTTP on port %s ...&apos; % PORT while True: client_connection, client_address = listen_socket.accept() request = client_connection.recv(1024) print request http_response = &quot;&quot;&quot;\\ HTTP/1.1 200 OK Hello, World! &quot;&quot;&quot; client_connection.sendall(http_response) client_connection.close() 将上面的代码保存为webserver1.py，或者直接从我的Github仓库下载，然后通过命令行运行该文件： $ python webserver1.py Serving HTTP on port 8888 … 接下来，在浏览器的地址栏输入这个链接：http://localhost:8888/hello，然后按下回车键，你就会看见神奇的一幕。在浏览器中，应该会出现“Hello, World!”这句话： 是不是很神奇？接下来，我们来分析背后的实现原理。 首先，我们来看你所输入的网络地址。它的名字叫URL（Uniform Resource Locator，统一资源定位符），其基本结构如下： 通过URL，你告诉了浏览器它所需要发现并连接的网络服务器地址，以及获取服务器上的页面路径。不过在浏览器发送HTTP请求之前，它首先要与目标网络服务器建立TCP连接。然后，浏览器再通过TCP连接发送HTTP请求至服务器，并等待服务器返回HTTP响应。当浏览器收到响应的时候，就会在页面上显示响应的内容，而在上面的例子中,浏览器显示的就是“Hello, World!”这句话。 那么，在客户端发送请求、服务器返回响应之前，二者究竟是如何建立起TCP连接的呢？要建立起TCP连接，服务器和客户端都使用了所谓的套接字（socket）。接下来，我们不直接使用浏览器，而是在命令行使用telnet手动模拟浏览器。 在运行网络服务器的同一台电脑商，通过命令行开启一次telnet会话，将需要连接的主机设置为localhost，主机的连接端口设置为8888，然后按回车键： $ telnet localhost 8888 Trying 127.0.0.1 … Connected to localhost. 完成这些操作之后，你其实已经与本地运行的网络服务器建立了TCP连接，随时可以发送和接收HTTP信息。在下面这张图片里，展示的是服务器接受新TCP连接所需要完成的标准流程。 在上面那个telnet会话中，我们输入GET /hello HTTP/1.1，然后按下回车： $ telnet localhost 8888 Trying 127.0.0.1 … Connected to localhost. GET /hello HTTP/1.1 HTTP/1.1 200 OK Hello, World! 你成功地手动模拟了浏览器！你手动发送了一条HTTP请求，然后收到了HTTP响应。下面这幅图展示的是HTTP请求的基本结构： HTTP请求行包括了HTTP方法（这里使用的是GET方法，因为我们希望从服务器获取内容），服务器页面路径（/hello）以及HTTP协议的版本。 为了尽量简化，我们目前实现的网络服务器并不会解析上面的请求，你完全可以输入一些没有任何意义的代码，也一样可以收到”Hello, World!”响应。 在你输入请求代码并按下回车键之后，客户端就将该请求发送至服务器了，服务器则会解析你发送的请求，并返回相应的HTTP响应。 下面这张图显示的是服务器返回至客户端的HTTP响应详情： 我们来分析一下。响应中包含了状态行HTTP/1.1 200 OK，之后是必须的空行，然后是HTTP响应的正文。 响应的状态行HTTP/1.1 200 OK中，包含了HTTP版本、HTTP状态码以及与状态码相对应的原因短语（Reason Phrase）。浏览器收到响应之后，会显示响应的正文，这就是为什么你会在浏览器中看到“Hello, World!”这句话。 这就是网络服务器基本的工作原理了。简单回顾一下：网络服务器首先创建一个侦听套接字（listening socket），并开启一个永续循环接收新连接；客户端启动一个与服务器的TCP连接，成功建立连接之后，向服务器发送HTTP请求，之后服务器返回HTTP响应。要建立TCP连接，客户端和服务器都使用了套接字。 现在，你已经拥有了一个基本可用的简易网络服务器，你可以使用浏览器或其他HTTP客户端进行测试。正如上文所展示的，通过telnet命令并手动输入HTTP请求，你自己也可以成为一个HTTP客户端。 下面给大家布置一道思考题：如何在不对服务器代码作任何修改的情况下，通过该服务器运行Djando应用、Flask应用和Pyramid应用，同时满足这些不同网络框架的要求？ 答案将在《自己动手开发网络服务器》系列文章的第二部分揭晓。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"}]},{"title":"LEGB规则","slug":"LEGB规则","date":"2017-04-12T16:00:00.000Z","updated":"2018-03-18T13:03:37.837Z","comments":true,"path":"2017/04/13/LEGB规则/","link":"","permalink":"http://yoursite.com/2017/04/13/LEGB规则/","excerpt":"","text":"—————————————————————————————————————————————————转自：https://foofish.net/python-legb.html 作者：FOOFISH-PYTHON之禅—————————————————————————————————————————————————理解LEGB前，首先需要对Python的作用域、命名空间有一定的了解，话题才能继续展开。 命名空间 命名空间表示变量的可见范围，一个变量名可以定义在多个不同的命名空间，相互之间并不冲突，但同一个命名空间中不能有两个相同的变量名。比如：两个叫“张三”的学生可以同时存在于班级A和班级B中，如果两个张三都是一个班级，那么带来的麻烦复杂很多了，在Python中你不能这么干。 在Python中用字典来表示一个命名空间，命名空间中保存了变量（名字）和对象的映射关系，在Python中命名空间出现在哪些地方呢？有函数范围内的命名空间（local），有模块范围内的命名空间（global），有python内建的命名空间（built-in），还有类对象的所有属性组成的命名空间。 命名空间的生命周期 所有的命名空间都是有生命周期的，对于python内建的命名空间，python解析器启动时创建，一直保留直至直python解析器退出时才消亡。而对于函数的local命名空间是在函数每次被调用的时候创建，调用完成函数返回时消亡，而对于模块的global命名空间是在该模块被import的时候创建，解析器退出时消亡。 作用域 一个作用域是指一段程序的正文区域，可以是一个函数或一段代码。一个变量的作用域是指该变量的有效范围。Python的作用域是静态作用域，因为它是由代码中得位置决定的，而命名空间就是作用域的动态表现。 LGB Python2.2之前定义了三个作用域，分别是： global作用域，对应的global命名空间，一个模块最外层定义的一个作用域。 local作用域，对应local命名空间，由函数定义的。 builtin作用域，对应builtin命名空间，python内部定义的最顶层的作用域，在这个作用域里面定义了各种内建函数：open、range、xrange、list等等。 那时的Python作用域规则叫做LEB规则，变量（名字）的引用按照local作用域、global作用域、builtin作用域的顺序来查找。 首先来看一段代码: a = 1 def foo(): a = 2 print a //[1] print a //[2] foo() [1]处输出结果为2，Python首先会在函数foo定义的local作用域中查找名字a，如果找到了直接输出，没有没找到就会在模块定义的global作用域中查找，如果还没找到，就到Python内建的builtin作用域中查找a，如果还没找到就报异常：NameError: name ‘a’ is not defined。引用过程如图： [2]处输出结果为1，查找顺序同样是按照LGB规则，只不过这里的local作用域就是global作用域。 LEGB规则Python2.2开始引入嵌套函数，嵌套函数为python提供了闭包实现。 a = 1 def foo(): a = 2 def bar(): print a //[1] return bar func = foo() func() 函数bar和a=2捆包在一起组成一个闭包，因此这里a=2即使脱离了foo所在的local作用域，但调用func的时候（其实就是调用bar）查找名字a的顺序是LEGB规则，这里的E就是enclosing的缩写，代表的“直接外围作用域”这个概念。查找a时，在bar对应的local作用域中没有时，然后在它外围的作用域中查找a。LEGB规定了查找一个名称的顺序为：local–&gt;enclosing–&gt;global–&gt;builtin。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"命名空间","slug":"命名空间","permalink":"http://yoursite.com/tags/命名空间/"},{"name":"作用域","slug":"作用域","permalink":"http://yoursite.com/tags/作用域/"},{"name":"LEGB","slug":"LEGB","permalink":"http://yoursite.com/tags/LEGB/"}]},{"title":"python 中locals() 和 globals()","slug":"locals() 和 globals()","date":"2017-04-11T16:00:00.000Z","updated":"2018-03-08T15:10:36.192Z","comments":true,"path":"2017/04/12/locals() 和 globals()/","link":"","permalink":"http://yoursite.com/2017/04/12/locals() 和 globals()/","excerpt":"","text":"1、locals() 和 globals() 是python 的内建函数，他们提供了字典的形式访问局部变量和全局变量的方式。 def test(arg): a=1 b=2 data_dict = {} print locals() print globals() if __name__ == &apos;__main__&apos;: test(3) 输出为： {&apos;data_dict&apos;: {}, &apos;b&apos;: 2, &apos;a&apos;: 1, &apos;arg&apos;: 3} {&apos;__name__&apos;: &apos;__main__&apos;, &apos;__doc__&apos;: None, &apos;__package__&apos;: None, &apos;__loader__&apos;: &lt;class &apos;_frozen_importlib.BuiltinImporter&apos;&gt;, &apos;__spec__&apos;: None, &apos;__annotations__&apos;: {}, &apos;__builtins__&apos;: &lt;module &apos;builtins&apos; (built-in)&gt;, &apos;__file__&apos;: &apos;C:/Users/闫瑞龙/Desktop/demo/locals.py&apos;, &apos;test&apos;: &lt;function test at 0x000001E26680C598&gt;} 2、locals() 返回是当前局部变量的深拷贝，修改locals() 中变量值的时候，实际上对于原变量本身是没有任何影响的。而globals()返回的是全局变量的字典，修改其中的内容，值会真正的发生改变。示例代码： b = 5 # 定义一个全局变量 def test2(): a=1 locals()[&quot;a&quot;] = 2 # 修改局部变量 print &quot;a=&quot;, a globals()[&quot;b&quot;] = 6 # 修改全局变量 print &quot;b=&quot;, b if __name__ == &apos;__main__&apos;: test2() 输出为： a= 1 b= 6","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"内置函数","slug":"内置函数","permalink":"http://yoursite.com/tags/内置函数/"}]},{"title":"作用域","slug":"作用域","date":"2017-04-10T16:00:00.000Z","updated":"2018-03-18T13:03:52.691Z","comments":true,"path":"2017/04/11/作用域/","link":"","permalink":"http://yoursite.com/2017/04/11/作用域/","excerpt":"","text":"LEGB原则 python中作用域有四种： L （Local） 局部作用域 E （Enclosing） 闭包函数外的函数中 G （Global） 全局作用域 B （Built-in） 内建作用域 python按照LEGB原则搜索变量，即优先级L&gt;E&gt;G&gt;B。 dir = 1 # Global def outer(): dir = 2 # Enclosing def inner(): dir = 3 # Local return dir return inner print outer() # 输出3 作用域（Scope）和命名空间（NameSpace） def/lambda会创建新的作用域，生成器表达式都有引入新的作用域，class的定义没有作用域，只是创建一个隔离的命名空间。在Python中，scope是由namespace按特定的层级结构组合起来的。scope一定是namespace，但namespace不一定是scope。命名空间跟作用域的区别是，它不能在里面再嵌套其他作用域。下面看两个例子。 例1： a = 1 def test(): a += 1 a = 2 test() #异常 UnboundLocalError: local variable ‘a’ referenced before assignment。这是因为解释器看到a+=1时，按照LEGB优先在Local中找到了a的声明，执行时先a+=1在a=2声明之前，所以抛出异常。 例2： class A(object): x = 2 gen = (x*i for i in xrange(5)) if __name__ == &quot;__main__&quot;: a = A() print list(a.gen)#异常 上面的代码会抛出异常：NameError: global name ‘x’ is not defined。这是因为gen = （x for _ in xrange(5）是生成器，会产生新的作用域。而classA 中并不产生作用域。按照LEGB原则，不能找到x的定义，所以抛出异常。解决这个问题有几种方案。 1，将x定义为全局变量，这样可以解决异常，但是可能违背了类的逻辑。 2，将生成器表达式改为列表表达式。 gen = [x*i for i in xrange(5)] 在python2中，列表表达式不产生新的作用域，所以不会抛出异常。但是在python3中仍有异常。 3，用A.x的方式访问类属性。 gen = (A.x*i for i in xrange(5)) 4，引入lambda函数，将class命名空间的x作为变量传入到匿名函数中。 gen = (lambda x: (x*i for i in xrange(5)))(x) 这个问题可以理解为class不能产生作用域导致的，在函数中就没有这个问题。 def test(): x = 2 gen = (i * x for i in xrange(5)) return gen gen = test() print list(gen)#输出[0, 2, 4, 6, 8]","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"作用域","slug":"作用域","permalink":"http://yoursite.com/tags/作用域/"},{"name":"内建函数","slug":"内建函数","permalink":"http://yoursite.com/tags/内建函数/"}]},{"title":"如何避免循环导入","slug":"避免循环导入","date":"2017-04-09T16:00:00.000Z","updated":"2018-03-08T15:08:16.099Z","comments":true,"path":"2017/04/10/避免循环导入/","link":"","permalink":"http://yoursite.com/2017/04/10/避免循环导入/","excerpt":"","text":"1，什么是循环导入 a.py from b import b print &apos;---------this is module a.py----------&apos; def a(): print(&quot;hello, a&quot;) b() a() b.py from a import a print &apos;----------this is module b.py----------&apos; def b(): print(&quot;hello, b&quot;) def c(): a() c() 运行pyhon a.py Traceback (most recent call last): File &quot;a.py&quot;, line 1, in &lt;module&gt; from b import b File &quot;/home/yan/py_demo/01-python高级-1/b.py&quot;, line 1, in &lt;module&gt; from a import a File &quot;/home/yan/py_demo/01-python高级-1/a.py&quot;, line 1, in &lt;module&gt; from b import b ImportError: cannot import name b 怎样避免循环导入 1,例如放在函数体内导入 a.py print(&apos;---------this is module a.py----------&apos;) def a(): print(&quot;hello, a&quot;) from b import b b() a() b.py print(&apos;----------this is module b.py----------&apos;) def b(): print(&quot;hello, b&quot;) def c(): from a import a a() c() 2，程序设计上分层，降低耦合","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"模块","slug":"模块","permalink":"http://yoursite.com/tags/模块/"}]},{"title":"模块导入","slug":"模块导入","date":"2017-04-06T16:00:00.000Z","updated":"2018-03-08T15:05:25.952Z","comments":true,"path":"2017/04/07/模块导入/","link":"","permalink":"http://yoursite.com/2017/04/07/模块导入/","excerpt":"","text":"在python中，每一个以 .py结尾的Python文件都是一个模块。其他的文件可以通过导入一个模块来读取该模块的内容。导入从本质上来讲，就是载入另一个文件，并能够读取那个文件的内容。一个模块的内容通过这样的属性能够被外部世界使用。 这种基于模块的方式使模块变成了Python程序架构的一个核心概念。更大的程序往往以多个模块文件的形式出现，并且导入了其他模块文件的工具。其中的一个模块文件被设计成主文件，或叫做顶层文件（就是那个启动后能够运行整个程序的文件）。 默认情况下，模块在第一次被导入之后，其他的导入都不再有效。如果此时在另一个窗口中改变并保存了模块的源代码文件，也无法更新该模块。这样设计的原因在于，导入是一个开销很大的操作（导入必须找到文件，将其编译成字节码，并且运行代码），以至于每个文件、每个程序运行不能够重复多于一次。 import 搜索路径 从下面列出的目录里依次查找要导入的模块文件 ‘’表示当前路径 In [1]: import sys In [2]: sys.path Out[2]: [&apos;&apos;, &apos;/usr/bin&apos;, &apos;/usr/lib/python35.zip&apos;, &apos;/usr/lib/python3.5&apos;, &apos;/usr/lib/python3.5/plat-x86_64-linux-gnu&apos;, &apos;/usr/lib/python3.5/lib-dynload&apos;, &apos;/usr/local/lib/python3.5/dist-packages&apos;, &apos;/usr/lib/python3/dist-packages&apos;, &apos;/usr/lib/python3/dist-packages/IPython/extensions&apos;, &apos;/home/yan/.ipython&apos;] 动态添加模块搜索路径 In [3]: sys.path.append(&apos;/home&apos;) In [4]: sys.path Out[4]: [&apos;&apos;, &apos;/usr/bin&apos;, &apos;/usr/lib/python35.zip&apos;, &apos;/usr/lib/python3.5&apos;, &apos;/usr/lib/python3.5/plat-x86_64-linux-gnu&apos;, &apos;/usr/lib/python3.5/lib-dynload&apos;, &apos;/usr/local/lib/python3.5/dist-packages&apos;, &apos;/usr/lib/python3/dist-packages&apos;, &apos;/usr/lib/python3/dist-packages/IPython/extensions&apos;, &apos;/home/yan/.ipython&apos;, &apos;/home&apos;] 设置模块路径搜索优先级 In [5]: sys.path.insert(0, &apos;/home/itcast/xxx&apos;) #可以确保先搜索这个路径 注： 程序向sys.path添加的目录只会在此程序的生命周期之内有效，其他所有的对sys.path的动态操作也是如此。 重新导入模块 模块被导入后，import module不能重新导入模块，重新导入需用 测试模块内容 test.py def test(): print(&apos;test&apos;) In [1]: import test In [2]: test.test() test 修改测试模块 def test(): print(&apos;test&apos;) print(&apos;test-1&apos;) 重新加载模块 In [3]: from imp import * In [4]: reload(test) Out[4]: &lt;module &apos;test&apos; from &apos;/home/yan/py_demo/01-python高级-1/test.py&apos;&gt; In [5]: test.test() test test-1 注意：reload函数希望获得的参数是一个已经加载了的模块对象的名称，所以如果在重载之前,请确保已经成功地导入了这个模块。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"模块","slug":"模块","permalink":"http://yoursite.com/tags/模块/"}]},{"title":"python进程和线程1","slug":"python进程和线程1","date":"2017-04-04T16:00:00.000Z","updated":"2018-03-18T13:04:18.417Z","comments":true,"path":"2017/04/05/python进程和线程1/","link":"","permalink":"http://yoursite.com/2017/04/05/python进程和线程1/","excerpt":"","text":"转自廖雪峰官网 我们介绍了多进程和多线程，这是实现多任务最常用的两种方式。现在，我们来讨论一下这两种方式的优缺点。 首先，要实现多任务，通常我们会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。 如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。 如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。 多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。 多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。 在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。 线程切换无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？ 我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。 如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型，或者批处理任务模型。 假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。 但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。 所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。 计算密集型 vs. IO密集型是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。 第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 异步IO考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。 现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。 对应到Python语言，单进程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"线程","slug":"线程","permalink":"http://yoursite.com/tags/线程/"},{"name":"进程","slug":"进程","permalink":"http://yoursite.com/tags/进程/"}]},{"title":"Python之线程、进程和协程2","slug":"Python之线程、进程和协程2","date":"2017-03-30T16:00:00.000Z","updated":"2018-03-18T13:07:41.775Z","comments":true,"path":"2017/03/31/Python之线程、进程和协程2/","link":"","permalink":"http://yoursite.com/2017/03/31/Python之线程、进程和协程2/","excerpt":"","text":"原文出处： 银河系1234目录： 引言一、线程1.1 普通的多线程 1.2 自定义线程类 1.3 线程锁 1.3.1 未使用锁 1.3.2 普通锁Lock和RLock 1.3.3 信号量(Semaphore) 1.3.4 事件(Event) 1.3.5 条件(condition) 1.3 全局解释器锁（GIL） 1.4 定时器（Timer） 1.5 队列 1.5.1 Queue：先进先出队列 1.5.2 LifoQueue：后进先出队列 1.5.3 PriorityQueue：优先级队列 1.5.4 deque：双向队列 1.6 生产者消费者模型 1.7 线程池 二、进程2.1 进程的数据共享 2.1.1 使用Array共享数据 2.1.2 使用Manager共享数据 2.1.3 使用queues的Queue类共享数据 2.2 进程锁 2.3 进程池 三、协程3.1 greenlet3.2 gevent 引言解释器环境：python3.5.1我们都知道python网络编程的两大必学模块socket和socketserver，其中的socketserver是一个支持IO多路复用和多线程、多进程的模块。一般我们在socketserver服务端代码中都会写这么一句： server = socketserver.ThreadingTCPServer(settings.IP_PORT, MyServer) ThreadingTCPServer这个类是一个支持多线程和TCP协议的socketserver，它的继承关系是这样的： class ThreadingTCPServer(ThreadingMixIn, TCPServer): pass 右边的TCPServer实际上是它主要的功能父类，而左边的ThreadingMixIn则是实现了多线程的类，它自己本身则没有任何代码。 MixIn在python的类命名中，很常见，一般被称为“混入”，戏称“乱入”，通常为了某种重要功能被子类继承。 class ThreadingMixIn: daemon_threads = False def process_request_thread(self, request, client_address): try: self.finish_request(request, client_address) self.shutdown_request(request) except: self.handle_error(request, client_address) self.shutdown_request(request) def process_request(self, request, client_address): t = threading.Thread(target = self.process_request_thread, args = (request, client_address)) t.daemon = self.daemon_threads t.start() 在ThreadingMixIn类中，其实就定义了一个属性，两个方法。在process_request方法中实际调用的正是python内置的多线程模块threading。这个模块是python中所有多线程的基础，socketserver本质上也是利用了这个模块。 一、线程线程，有时被称为轻量级进程(Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID，当前指令指针(PC），寄存器集合和堆栈组成。另外，线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不独立拥有系统资源，但它可与同属一个进程的其它线程共享该进程所拥有的全部资源。一个线程可以创建和撤消另一个线程，同一进程中的多个线程之间可以并发执行。由于线程之间的相互制约，致使线程在运行中呈现出间断性。线程也有就绪、阻塞和运行三种基本状态。就绪状态是指线程具备运行的所有条件，逻辑上可以运行，在等待处理机；运行状态是指线程占有处理机正在运行；阻塞状态是指线程在等待一个事件（如某个信号量），逻辑上不可执行。每一个应用程序都至少有一个进程和一个线程。线程是程序中一个单一的顺序控制流程。在单个程序中同时运行多个线程完成不同的被划分成一块一块的工作，称为多线程。以上那一段，可以不用看！举个例子，厂家要生产某个产品，在它的生产基地建设了很多厂房，每个厂房内又有多条流水生产线。所有厂房配合将整个产品生产出来，某个厂房内的所有流水线将这个厂房负责的产品部分生产出来。每个厂房拥有自己的材料库，厂房内的生产线共享这些材料。而每一个厂家要实现生产必须拥有至少一个厂房一条生产线。那么这个厂家就是某个应用程序；每个厂房就是一个进程；每条生产线都是一个线程。 1.1 普通的多线程在python中，threading模块提供线程的功能。通过它，我们可以轻易的在进程中创建多个线程。下面是个例子： import threading import time def show(arg): time.sleep(1) print(&apos;thread&apos;+str(arg)) for i in range(10): t = threading.Thread(target=show, args=(i,)) t.start() print(&apos;main thread stop&apos;) 上述代码创建了10个“前台”线程，然后控制器就交给了CPU，CPU根据指定算法进行调度，分片执行指令。下面是Thread类的主要方法： start 线程准备就绪，等待CPU调度 setName 为线程设置名称 getName 获取线程名称 setDaemon 设置为后台线程或前台线程（默认是False，前台线程） 如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，均停止。如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程也执行完成后，程序停止。 join 该方法非常重要。它的存在是告诉主线程，必须在这个位置等待子线程执行完毕后，才继续进行主线程的后面的代码。但是当setDaemon为True时，join方法是无效的。 run 线程被cpu调度后自动执行线程对象的run方法 1.2 自定义线程类 对于threading模块中的Thread类，本质上是执行了它的run方法。因此可以自定义线程类，让它继承Thread类，然后重写run方法。 import threading class MyThreading(threading.Thread): def __init__(self,func,arg): super(MyThreading,self).__init__() self.func = func self.arg = arg def run(self): self.func(self.arg) def f1(args): print(args) obj = MyThreading(f1, 123) obj.start() 1.3 线程锁 CPU执行任务时，在线程之间是进行随机调度的，并且每个线程可能只执行n条代码后就转而执行另外一条线程。由于在一个进程中的多个线程之间是共享资源和数据的，这就容易造成资源抢夺或脏数据，于是就有了锁的概念，限制某一时刻只有一个线程能访问某个指定的数据。 1.3.1 未使用锁 #!/usr/bin/env python # -*- coding:utf-8 -*- import threading import time NUM = 0 def show(): global NUM NUM += 1 name = t.getName() time.sleep(1) # 注意，这行语句的位置很重要，必须在NUM被修改后，否则观察不到脏数据的现象。 print(name, &quot;执行完毕后，NUM的值为： &quot;, NUM) for i in range(10): t = threading.Thread(target=show) t.start() print(&apos;main thread stop&apos;) 上述代码运行后，结果如下： main thread stop Thread-1 执行完毕后，NUM的值为： 10 Thread-2 执行完毕后，NUM的值为： 10 Thread-4 执行完毕后，NUM的值为： 10 Thread-9 执行完毕后，NUM的值为： 10 Thread-3 执行完毕后，NUM的值为： 10 Thread-6 执行完毕后，NUM的值为： 10 Thread-8 执行完毕后，NUM的值为： 10 Thread-7 执行完毕后，NUM的值为： 10 Thread-5 执行完毕后，NUM的值为： 10 Thread-10 执行完毕后，NUM的值为： 10 由此可见，由于线程同时访问一个数据，产生了错误的结果。为了解决这个问题，python在threading模块中定义了几种线程锁类，分别是： Lock 普通锁（不可嵌套） RLock 普通锁（可嵌套）常用 Semaphore 信号量 event 事件 condition 条件 1.3.2 普通锁Lock和RLock 类名：Lock或RLock 普通锁，也叫互斥锁，是独占的，同一时刻只有一个线程被放行。 import time import threading NUM = 10 def func(lock): global NUM lock.acquire() # 让锁开始起作用 NUM -= 1 time.sleep(1) print(NUM) lock.release() # 释放锁 lock = threading.Lock() # 实例化一个锁对象 for i in range(10): t = threading.Thread(target=func, args=(lock,)) # 记得把锁当作参数传递给func参数 t.start() 以上是threading模块的Lock类，它不支持嵌套锁。RLcok类的用法和Lock一模一样，但它支持嵌套，因此我们一般直接使用RLcok类。 1.3.3 信号量(Semaphore) 类名：BoundedSemaphore 这种锁允许一定数量的线程同时更改数据，它不是互斥锁。比如地铁安检，排队人很多，工作人员只允许一定数量的人进入安检区，其它的人继续排队。 #!/usr/bin/env python # -*- coding:utf-8 -*- import time import threading def run(n): semaphore.acquire() print(&quot;run the thread: %s&quot; % n) time.sleep(1) semaphore.release() num = 0 semaphore = threading.BoundedSemaphore(5) # 最多允许5个线程同时运行 for i in range(20): t = threading.Thread(target=run, args=(i,)) t.start() 1.3.4 事件(Event) 类名：Event 事件主要提供了三个方法 set、wait、clear。 事件机制：全局定义了一个“Flag”，如果“Flag”的值为False，那么当程序执行wait方法时就会阻塞，如果“Flag”值为True，那么wait方法时便不再阻塞。这种锁，类似交通红绿灯（默认是红灯），它属于在红灯的时候一次性阻挡所有线程，在绿灯的时候，一次性放行所有的排队中的线程。 clear：将“Flag”设置为False set：将“Flag”设置为True import threading def func(e,i): print(i) e.wait() # 检测当前event是什么状态，如果是红灯，则阻塞，如果是绿灯则继续往下执行。默认是红灯。 print(i+100) event = threading.Event() for i in range(10): t = threading.Thread(target=func, args=(event, i)) t.start() event.clear() # 主动将状态设置为红灯 inp = input(&quot;&gt;&gt;&gt;&quot;) if inp == &quot;1&quot;: event.set() # 主动将状态设置为绿灯 1.3.5 条件(condition) 类名：Condition 该机制会使得线程等待，只有满足某条件时，才释放n个线程。 import threading def condition(): ret = False r = input(&quot;&gt;&gt;&gt;&quot;) if r == &quot;yes&quot;: ret = True return ret def func(conn, i): print(i) conn.acquire() conn.wait_for(condition) # 这个方法接受一个函数的返回值 print(i+100) conn.release() c = threading.Condition() for i in range(10): t = threading.Thread(target=func, args=(c, i,)) t.start() 上面的例子，每输入一次“yes”放行了一个线程。下面这个，可以选择一次放行几个线程。 #!/usr/bin/env python # -*- coding:utf-8 -*- import threading def run(n): con.acquire() con.wait() print(&quot;run the thread: %s&quot; %n) con.release() if __name__ == &apos;__main__&apos;: con = threading.Condition() for i in range(10): t = threading.Thread(target=run, args=(i,)) t.start() while True: inp = input(&apos;&gt;&gt;&gt;&apos;) if inp == &quot;q&quot;: break # 下面这三行是固定语法 con.acquire() con.notify(int(inp)) # 这个方法接收一个整数，表示让多少个线程通过 con.release() 1.3 全局解释器锁（GIL） 既然介绍了多线程和线程锁，那就不得不提及python的GIL，也就是全局解释器锁。在编程语言的世界，python因为GIL的问题广受诟病，因为它在解释器的层面限制了程序在同一时间只有一个线程被CPU实际执行，而不管你的程序里实际开了多少条线程。所以我们经常能发现，python中的多线程编程有时候效率还不如单线程，就是因为这个原因。那么，对于这个GIL，一些普遍的问题如下： 每种编程语言都有GIL吗？ 以python官方Cpython解释器为代表….其他语言好像未见。 为什么要有GIL？ 作为解释型语言，Python的解释器必须做到既安全又高效。我们都知道多线程编程会遇到的问题。解释器要留意的是避免在不同的线程操作内部共享的数据。同时它还要保证在管理用户线程时总是有最大化的计算资源。那么，不同线程同时访问时，数据的保护机制是怎样的呢？答案是解释器全局锁GIL。GIL对诸如当前线程状态和为垃圾回收而用的堆分配对象这样的东西的访问提供着保护。 为什么不能去掉GIL？ 首先，在早期的python解释器依赖较多的全局状态，传承下来，使得想要移除当今的GIL变得更加困难。其次，对于程序员而言，仅仅是想要理解它的实现就需要对操作系统设计、多线程编程、C语言、解释器设计和CPython解释器的实现有着非常彻底的理解。在1999年，针对Python1.5，一个“freethreading”补丁已经尝试移除GIL，用细粒度的锁来代替。然而，GIL的移除给单线程程序的执行速度带来了一定的负面影响。当用单线程执行时，速度大约降低了40%。虽然使用两个线程时在速度上得到了提高，但这个提高并没有随着核数的增加而线性增长。因此这个补丁没有被采纳。 另外，在python的不同解释器实现中，如PyPy就移除了GIL，其执行速度更快（不单单是去除GIL的原因）。然而，我们通常使用的CPython占有着统治地位的使用量，所以，你懂的。 在Python 3.2中实现了一个新的GIL，并且带着一些积极的结果。这是自1992年以来，GIL的一次最主要改变。旧的GIL通过对Python指令进行计数来确定何时放弃GIL。在新的GIL实现中，用一个固定的超时时间来指示当前的线程以放弃这个锁。在当前线程保持这个锁，且当第二个线程请求这个锁的时候，当前线程就会在5ms后被强制释放掉这个锁（这就是说，当前线程每5ms就要检查其是否需要释放这个锁）。当任务是可行的时候，这会使得线程间的切换更加可预测。 GIL对我们有什么影响？ 最大的影响是我们不能随意使用多线程。要区分任务场景。 在单核cpu情况下对性能的影响可以忽略不计，多线程多进程都差不多。在多核CPU时，多线程效率较低。GIL对单进程和多进程没有影响。 在实际使用中有什么好的建议？ 建议在IO密集型任务中使用多线程，在计算密集型任务中使用多进程。深入研究python的协程机制，你会有惊喜的。 更多的详细介绍和说明请参考下面的文献： 原文：Python’s Hardest Problem 译文：Python 最难的问题 1.4 定时器（Timer） 定时器，指定n秒后执行某操作。很简单但很使用的东西。 from threading import Timer def hello(): print(&quot;hello, world&quot;) t = Timer(1, hello) # 表示1秒后执行hello函数 t.start() 1.5 队列 通常而言，队列是一种先进先出的数据结构，与之对应的是堆栈这种后进先出的结构。但是在python中，它内置了一个queue模块，它不但提供普通的队列，还提供一些特殊的队列。具体如下： queue.Queue ：先进先出队列 queue.LifoQueue ：后进先出队列 queue.PriorityQueue ：优先级队列 queue.deque ：双向队列 1.5.1 Queue：先进先出队列 这是最常用也是最普遍的队列，先看一个例子。 import queue q = queue.Queue(5) q.put(11) q.put(22) q.put(33) print(q.get()) print(q.get()) print(q.get()) Queue类的参数和方法： maxsize 队列的最大元素个数，也就是queue.Queue(5)中的5。当队列内的元素达到这个值时，后来的元素默认会阻塞，等待队列腾出位置。 def __init__(self, maxsize=0): self.maxsize = maxsize self._init(maxsize) qsize() 获取当前队列中元素的个数，也就是队列的大小 empty() 判断当前队列是否为空，返回True或者False full() 判断当前队列是否已满，返回True或者False put(self, block=True, timeout=None) 往队列里放一个元素，默认是阻塞和无时间限制的。如果，block设置为False，则不阻塞，这时，如果队列是满的，放不进去，就会弹出异常。如果timeout设置为n秒，则会等待这个秒数后才put，如果put不进去则弹出异常。 get(self, block=True, timeout=None) 从队列里获取一个元素。参数和put是一样的意思。 join() 阻塞进程，直到所有任务完成，需要配合另一个方法task_done。 def join(self): with self.all_tasks_done: while self.unfinished_tasks: self.all_tasks_done.wait() task_done() 表示某个任务完成。每一条get语句后需要一条task_done。 import queue q = queue.Queue(5) q.put(11) q.put(22) print(q.get()) q.task_done() print(q.get()) q.task_done() q.join() 1.5.2 LifoQueue：后进先出队列 类似于“堆栈”，后进先出。也较常用。 import queue q = queue.LifoQueue() q.put(123) q.put(456) print(q.get()) 上述代码运行结果是：456 1.5.3 PriorityQueue：优先级队列 带有权重的队列，每个元素都是一个元组，前面的数字表示它的优先级，数字越小优先级越高，同样的优先级先进先出 q = queue.PriorityQueue() q.put((1,&quot;alex1&quot;)) q.put((1,&quot;alex2&quot;)) q.put((1,&quot;alex3&quot;)) q.put((3,&quot;alex3&quot;)) print(q.get()) 1.5.4 deque：双向队列 Queue和LifoQueue的“综合体”，双向进出。方法较多，使用复杂，慎用！ q = queue.deque() q.append(123) q.append(333) q.appendleft(456) q.pop() q.popleft() 1.6 生产者消费者模型 利用多线程和队列可以搭建一个生产者消费者模型，用于处理大并发的服务。 在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。 为什么要使用生产者和消费者模式 在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。 什么是生产者消费者模式 生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。 这个阻塞队列就是用来给生产者和消费者解耦的。纵观大多数设计模式，都会找一个第三者出来进行解耦，如工厂模式的第三者是工厂类，模板模式的第三者是模板类。在学习一些设计模式的过程中，如果先找到这个模式的第三者，能帮助我们快速熟悉一个设计模式。 以上摘自方腾飞的《聊聊并发——生产者消费者模式》 下面是一个简单的厨师做包子，顾客吃包子的例子。 #!/usr/bin/env python # -*- coding:utf-8 -*- # Author:Liu Jiang import time import queue import threading q = queue.Queue(10) def productor(i): while True: q.put(&quot;厨师 %s 做的包子！&quot;%i) time.sleep(2) def consumer(k): while True: print(&quot;顾客 %s 吃了一个 %s&quot;%(k,q.get())) time.sleep(1) for i in range(3): t = threading.Thread(target=productor,args=(i,)) t.start() for k in range(10): v = threading.Thread(target=consumer,args=(k,)) v.start() 1.7 线程池 在使用多线程处理任务时也不是线程越多越好，由于在切换线程的时候，需要切换上下文环境，依然会造成cpu的大量开销。为解决这个问题，线程池的概念被提出来了。预先创建好一个较为优化的数量的线程，让过来的任务立刻能够使用，就形成了线程池。在python中，没有内置的较好的线程池模块，需要自己实现或使用第三方模块。下面是一个简单的线程池： #!/usr/bin/env python # -*- coding:utf-8 -*- # Author:Liu Jiang import queue import time import threading class MyThreadPool: def __init__(self, maxsize=5): self.maxsize = maxsize self._q = queue.Queue(maxsize) for i in range(maxsize): self._q.put(threading.Thread) def get_thread(self): return self._q.get() def add_thread(self): self._q.put(threading.Thread) def task(i, pool): print(i) time.sleep(1) pool.add_thread() pool = MyThreadPool(5) for i in range(100): t = pool.get_thread() obj = t(target=task, args=(i,pool)) obj.start() 上面的例子是把线程类当做元素添加到队列内。实现方法比较糙，每个线程使用后就被抛弃，一开始就将线程开到满，因此性能较差。下面是一个相对好一点的例子，在这个例子中，队列里存放的不再是线程对象，而是任务对象，线程池也不是一开始就直接开辟所有线程，而是根据需要，逐步建立，直至池满。通过详细的代码注释，应该会有个清晰的理解。 #!/usr/bin/env python # -*- coding:utf-8 -*- &quot;&quot;&quot; 一个基于thread和queue的线程池，以任务为队列元素，动态创建线程，重复利用线程， 通过close和terminate方法关闭线程池。 &quot;&quot;&quot; import queue import threading import contextlib import time # 创建空对象,用于停止线程 StopEvent = object() def callback(status, result): &quot;&quot;&quot; 根据需要进行的回调函数，默认不执行。 :param status: action函数的执行状态 :param result: action函数的返回值 :return: &quot;&quot;&quot; pass def action(thread_name,arg): &quot;&quot;&quot; 真实的任务定义在这个函数里 :param thread_name: 执行该方法的线程名 :param arg: 该函数需要的参数 :return: &quot;&quot;&quot; # 模拟该函数执行了0.1秒 time.sleep(0.1) print(&quot;第%s个任务调用了线程 %s，并打印了这条信息！&quot; % (arg+1, thread_name)) class ThreadPool: def __init__(self, max_num, max_task_num=None): &quot;&quot;&quot; 初始化线程池 :param max_num: 线程池最大线程数量 :param max_task_num: 任务队列长度 &quot;&quot;&quot; # 如果提供了最大任务数的参数，则将队列的最大元素个数设置为这个值。 if max_task_num: self.q = queue.Queue(max_task_num) # 默认队列可接受无限多个的任务 else: self.q = queue.Queue() # 设置线程池最多可实例化的线程数 self.max_num = max_num # 任务取消标识 self.cancel = False # 任务中断标识 self.terminal = False # 已实例化的线程列表 self.generate_list = [] # 处于空闲状态的线程列表 self.free_list = [] def put(self, func, args, callback=None): &quot;&quot;&quot; 往任务队列里放入一个任务 :param func: 任务函数 :param args: 任务函数所需参数 :param callback: 任务执行失败或成功后执行的回调函数，回调函数有两个参数 1、任务函数执行状态；2、任务函数返回值（默认为None，即：不执行回调函数） :return: 如果线程池已经终止，则返回True否则None &quot;&quot;&quot; # 先判断标识，看看任务是否取消了 if self.cancel: return # 如果没有空闲的线程，并且已创建的线程的数量小于预定义的最大线程数，则创建新线程。 if len(self.free_list) == 0 and len(self.generate_list) self.max_num: self.generate_thread() # 构造任务参数元组，分别是调用的函数，该函数的参数，回调函数。 w = (func, args, callback,) # 将任务放入队列 self.q.put(w) def generate_thread(self): &quot;&quot;&quot; 创建一个线程 &quot;&quot;&quot; # 每个线程都执行call方法 t = threading.Thread(target=self.call) t.start() def call(self): &quot;&quot;&quot; 循环去获取任务函数并执行任务函数。在正常情况下，每个线程都保存生存状态， 直到获取线程终止的flag。 &quot;&quot;&quot; # 获取当前线程的名字 current_thread = threading.currentThread().getName() # 将当前线程的名字加入已实例化的线程列表中 self.generate_list.append(current_thread) # 从任务队列中获取一个任务 event = self.q.get() # 让获取的任务不是终止线程的标识对象时 while event != StopEvent: # 解析任务中封装的三个参数 func, arguments, callback = event # 抓取异常，防止线程因为异常退出 try: # 正常执行任务函数 result = func(current_thread, *arguments) success = True except Exception as e: # 当任务执行过程中弹出异常 result = None success = False # 如果有指定的回调函数 if callback is not None: # 执行回调函数，并抓取异常 try: callback(success, result) except Exception as e: pass # 当某个线程正常执行完一个任务时，先执行worker_state方法 with self.worker_state(self.free_list, current_thread): # 如果强制关闭线程的flag开启，则传入一个StopEvent元素 if self.terminal: event = StopEvent # 否则获取一个正常的任务，并回调worker_state方法的yield语句 else: # 从这里开始又是一个正常的任务循环 event = self.q.get() else: # 一旦发现任务是个终止线程的标识元素，将线程从已创建线程列表中删除 self.generate_list.remove(current_thread) def close(self): &quot;&quot;&quot; 执行完所有的任务后，让所有线程都停止的方法 &quot;&quot;&quot; # 设置flag self.cancel = True # 计算已创建线程列表中线程的个数，然后往任务队列里推送相同数量的终止线程的标识元素 full_size = len(self.generate_list) while full_size: self.q.put(StopEvent) full_size -= 1 def terminate(self): &quot;&quot;&quot; 在任务执行过程中，终止线程，提前退出。 &quot;&quot;&quot; self.terminal = True # 强制性的停止线程 while self.generate_list: self.q.put(StopEvent) # 该装饰器用于上下文管理 @contextlib.contextmanager def worker_state(self, state_list, worker_thread): &quot;&quot;&quot; 用于记录空闲的线程，或从空闲列表中取出线程处理任务 &quot;&quot;&quot; # 将当前线程，添加到空闲线程列表中 state_list.append(worker_thread) # 捕获异常 try: # 在此等待 yield finally: # 将线程从空闲列表中移除 state_list.remove(worker_thread) # 调用方式 if __name__ == &apos;__main__&apos;: # 创建一个最多包含5个线程的线程池 pool = ThreadPool(5) # 创建100个任务，让线程池进行处理 for i in range(100): pool.put(action, (i,), callback) # 等待一定时间，让线程执行任务 time.sleep(3) print(&quot;-&quot; * 50) print(&quot;33[32;0m任务停止之前线程池中有%s个线程，空闲的线程有%s个！33[0m&quot; % (len(pool.generate_list), len(pool.free_list))) # 正常关闭线程池 pool.close() print(&quot;任务执行完毕，正常退出！&quot;) # 强制关闭线程池 # pool.terminate() # print(&quot;强制停止任务！&quot;) 二、进程 在python中multiprocess模块提供了Process类，实现进程相关的功能。但是，由于它是基于fork机制的，因此不被windows平台支持。想要在windows中运行，必须使用if name == ‘main:的方式，显然这只能用于调试和学习，不能用于实际环境。 （PS：在这里我必须吐槽一下python的包、模块和类的组织结构。在multiprocess中你既可以import大写的Process，也可以import小写的process，这两者是完全不同的东西。这种情况在python中很多，新手容易傻傻分不清。） 下面是一个简单的多进程例子，你会发现Process的用法和Thread的用法几乎一模一样。 from multiprocessing import Process def foo(i): print(&quot;This is Process &quot;, i) if __name__ == &apos;__main__&apos;: for i in range(5): p = Process(target=foo, args=(i,)) p.start() 2.1 进程的数据共享 每个进程都有自己独立的数据空间，不同进程之间通常是不能共享数据，创建一个进程需要非常大的开销。 from multiprocessing import Process list_1 = [] def foo(i): list_1.append(i) print(&quot;This is Process &quot;, i,&quot; and list_1 is &quot;, list_1) if __name__ == &apos;__main__&apos;: for i in range(5): p = Process(target=foo, args=(i,)) p.start() print(&quot;The end of list_1:&quot;, list_1) 运行上面的代码，你会发现列表list_1在各个进程中只有自己的数据，完全无法共享。想要进程之间进行资源共享可以使用queues/Array/Manager这三个multiprocess模块提供的类。 2.1.1 使用Array共享数据 from multiprocessing import Process from multiprocessing import Array def Foo(i,temp): temp[0] += 100 for item in temp: print(i,&apos;-----&gt;&apos;,item) if __name__ == &apos;__main__&apos;: temp = Array(&apos;i&apos;, [11, 22, 33, 44]) for i in range(2): p = Process(target=Foo, args=(i,temp)) p.start() 对于Array数组类，括号内的“i”表示它内部的元素全部是int类型，而不是指字符i，列表内的元素可以预先指定，也可以指定列表长度。概括的来说就是Array类在实例化的时候就必须指定数组的数据类型和数组的大小，类似temp = Array(‘i’, 5)。对于数据类型有下面的表格对应： ‘c’: ctypes.c_char, ‘u’: ctypes.c_wchar, ‘b’: ctypes.c_byte, ‘B’: ctypes.c_ubyte, ‘h’: ctypes.c_short, ‘H’: ctypes.c_ushort, ‘i’: ctypes.c_int, ‘I’: ctypes.c_uint, ‘l’: ctypes.c_long, ‘L’: ctypes.c_ulong, ‘f’: ctypes.c_float, ‘d’: ctypes.c_double 2.1.2 使用Manager共享数据 from multiprocessing import Process,Manager def Foo(i,dic): dic[i] = 100+i print(dic.values()) if __name__ == &apos;__main__&apos;: manage = Manager() dic = manage.dict() for i in range(10): p = Process(target=Foo, args=(i,dic)) p.start() p.join() Manager比Array要好用一点，因为它可以同时保存多种类型的数据格式。 2.1.3 使用queues的Queue类共享数据 import multiprocessing from multiprocessing import Process from multiprocessing import queues def foo(i,arg): arg.put(i) print(&apos;The Process is &apos;, i, &quot;and the queue&apos;s size is &quot;, arg.qsize()) if __name__ == &quot;__main__&quot;: li = queues.Queue(20, ctx=multiprocessing) for i in range(10): p = Process(target=foo, args=(i,li,)) p.start() 这里就有点类似上面的队列了。从运行结果里，你还能发现数据共享中存在的脏数据问题。另外，比较悲催的是multiprocessing里还有一个Queue，一样能实现这个功能。 2.2 进程锁 为了防止和多线程一样的出现数据抢夺和脏数据的问题，同样需要设置进程锁。与threading类似，在multiprocessing里也有同名的锁类RLock, Lock, Event, Condition, Semaphore，连用法都是一样样的！（这个我喜欢） from multiprocessing import Process from multiprocessing import queues from multiprocessing import Array from multiprocessing import RLock, Lock, Event, Condition, Semaphore import multiprocessing import time def foo(i,lis,lc): lc.acquire() lis[0] = lis[0] - 1 time.sleep(1) print(&apos;say hi&apos;,lis[0]) lc.release() if __name__ == &quot;__main__&quot;: # li = [] li = Array(&apos;i&apos;, 1) li[0] = 10 lock = RLock() for i in range(10): p = Process(target=foo,args=(i,li,lock)) p.start() 2.3 进程池 既然有线程池，那必然也有进程池。但是，python给我们内置了一个进程池，不需要像线程池那样需要自定义，你只需要简单的from multiprocessing import Pool。 #!/usr/bin/env python # -*- coding:utf-8 -*- from multiprocessing import Pool import time def f1(args): time.sleep(1) print(args) if __name__ == &apos;__main__&apos;: p = Pool(5) for i in range(30): p.apply_async(func=f1, args= (i,)) p.close() # 等子进程执行完毕后关闭进程池 # time.sleep(2) # p.terminate() # 立刻关闭进程池 p.join() 进程池内部维护一个进程序列，当使用时，去进程池中获取一个进程，如果进程池序列中没有可供使用的进程，那么程序就会等待，直到进程池中有可用进程为止。进程池中有以下几个主要方法： apply：从进程池里取一个进程并执行 apply_async：apply的异步版本 terminate:立刻关闭进程池 join：主进程等待所有子进程执行完毕。必须在close或terminate之后。 close：等待所有进程结束后，才关闭进程池。 三、协程 线程和进程的操作是由程序触发系统接口，最后的执行者是系统，它本质上是操作系统提供的功能。而协程的操作则是程序员指定的，在python中通过yield，人为的实现并发处理。 协程存在的意义：对于多线程应用，CPU通过切片的方式来切换线程间的执行，线程切换时需要耗时。协程，则只使用一个线程，分解一个线程成为多个“微线程”，在一个线程中规定某个代码块的执行顺序。 协程的适用场景：当程序中存在大量不需要CPU的操作时（IO）。 在不需要自己“造轮子”的年代，同样有第三方模块为我们提供了高效的协程，这里介绍一下greenlet和gevent。本质上，gevent是对greenlet的高级封装，因此一般用它就行，这是一个相当高效的模块。 在使用它们之前，需要先安装，可以通过源码，也可以通过pip。 3.1 greenlet from greenlet import greenlet def test1(): print(12) gr2.switch() print(34) gr2.switch() def test2(): print(56) gr1.switch() print(78) gr1 = greenlet(test1) gr2 = greenlet(test2) gr1.switch() 实际上，greenlet就是通过switch方法在不同的任务之间进行切换。 3.2 gevent from gevent import monkey; monkey.patch_all() import gevent import requests def f(url): print(&apos;GET: %s&apos; % url) resp = requests.get(url) data = resp.text print(&apos;%d bytes received from %s.&apos; % (len(data), url)) gevent.joinall([ gevent.spawn(f, &apos;https://www.python.org/&apos;), gevent.spawn(f, &apos;https://www.yahoo.com/&apos;), gevent.spawn(f, &apos;https://github.com/&apos;), ]) 通过joinall将任务f和它的参数进行统一调度，实现单线程中的协程。代码封装层次很高，实际使用只需要了解它的几个主要方法即可。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"线程","slug":"线程","permalink":"http://yoursite.com/tags/线程/"},{"name":"进程","slug":"进程","permalink":"http://yoursite.com/tags/进程/"},{"name":"协程","slug":"协程","permalink":"http://yoursite.com/tags/协程/"},{"name":"队列","slug":"队列","permalink":"http://yoursite.com/tags/队列/"},{"name":"锁","slug":"锁","permalink":"http://yoursite.com/tags/锁/"}]},{"title":"python之==和is","slug":"等号和is","date":"2017-03-29T16:00:00.000Z","updated":"2018-03-06T12:57:46.356Z","comments":true,"path":"2017/03/30/等号和is/","link":"","permalink":"http://yoursite.com/2017/03/30/等号和is/","excerpt":"","text":"关于 == 和 is 判断内容的时候使用== #这里比较的是两个对象中的值，而不进行内存地址的检测，返回布尔值 &gt;&gt;&gt;a = [1,2,3] &gt;&gt;&gt;b = [1,2,3] &gt;&gt;&gt;a == b True 判断是否指向同一个对象的时候用is &gt;&gt;&gt;a = [1,2,3] &gt;&gt;&gt;b = [1,2,3] &gt;&gt;&gt; id(a) 45600072 &gt;&gt;&gt; id(b) 45721032 &gt;&gt;&gt; a is b False 这里需要注意的是： 1,整数在程序中的使用非常广泛，Python为了优化速度，使用了小整数对象池，避免为整数频繁申请和销毁内存空间。python对小整数的定义是[-5,257],也就是-5到256且包含256，这些整数对象是提前建立好的，在一个python的程序中，所有位于这个范围内的整数使用的都是同一个对象。同理，单个字母也是这样的。2,如果定义的变量都是普通字母，不包含其他特殊符号的，默认开启intern机制，共用对象，不会进行新建例子： &gt;&gt;&gt; a = 100 &gt;&gt;&gt; b = 100 &gt;&gt;&gt; a is b True &gt;&gt;&gt; a = &apos;hehe&apos; &gt;&gt;&gt; b = &apos;hehe&apos; &gt;&gt;&gt; a is b True","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"运算符","slug":"运算符","permalink":"http://yoursite.com/tags/运算符/"}]},{"title":"静态方法和类方法","slug":"静态方法和类方法","date":"2017-03-28T16:00:00.000Z","updated":"2018-03-06T12:57:39.738Z","comments":true,"path":"2017/03/29/静态方法和类方法/","link":"","permalink":"http://yoursite.com/2017/03/29/静态方法和类方法/","excerpt":"","text":"类方法是类对象所拥有的方法，需要用修饰器@classmethod来标识其为类方法，对于类方法，第一个参数必须是类对象，一般以cls作为第一个参数（当然可以用其他名称的变量作为其第一个参数，但是大部分人都习惯以’cls’作为第一个参数的名字，就最好用’cls’了），能够通过实例对象和类对象去访问。 class People(object): country = &apos;china&apos; #类方法，用classmethod来进行修饰 @classmethod def getCountry(cls): return cls.country p = People() print p.getCountry() #可以用过实例对象引用 print People.getCountry() #可以通过类对象引用 类方法还有一个用途就是可以对类属性进行修改： class People(object): country = &apos;china&apos; #类方法，用classmethod来进行修饰 @classmethod def getCountry(cls): return cls.country @classmethod def setCountry(cls,country): cls.country = country p = People() print p.getCountry() #可以用过实例对象引用 print People.getCountry() #可以通过类对象引用 p.setCountry(&apos;japan&apos;) print p.getCountry() print People.getCountry() #输出 &gt;&gt;&gt;china &gt;&gt;&gt;china &gt;&gt;&gt;japan &gt;&gt;&gt;japan 结果显示在用类方法对类属性修改之后，通过类对象和实例对象访问都发生了改变 静态方法需要通过修饰器@staticmethod来进行修饰，静态方法不需要多定义参数 class People(object): country = &apos;china&apos; @staticmethod #静态方法 def getCountry(): return People.country print People.getCountry() 总结 从类方法和实例方法以及静态方法的定义形式就可以看出来，类方法的第一个参数是类对象cls，那么通过cls引用的必定是类对象的属性和方法；而实例方法的第一个参数是实例对象self，那么通过self引用的可能是类属性、也有可能是实例属性（这个需要具体分析），不过在存在相同名称的类属性和实例属性的情况下，实例属性优先级更高。静态方法中不需要额外定义参数，因此在静态方法中引用类属性的话，必须通过类对象来引用","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"静态方法和类方法","slug":"静态方法和类方法","permalink":"http://yoursite.com/tags/静态方法和类方法/"}]},{"title":"单元测试","slug":"单元测试","date":"2017-03-27T16:00:00.000Z","updated":"2018-03-06T12:57:33.997Z","comments":true,"path":"2017/03/28/单元测试/","link":"","permalink":"http://yoursite.com/2017/03/28/单元测试/","excerpt":"","text":"转载自廖雪峰官网 如果你听说过“测试驱动开发”（TDD：Test-Driven Development），单元测试就不陌生。 单元测试是用来对一个模块、一个函数或者一个类来进行正确性检验的测试工作。 比如对函数abs()，我们可以编写出以下几个测试用例： 输入正数，比如1、1.2、0.99，期待返回值与输入相同； 输入负数，比如-1、-1.2、-0.99，期待返回值与输入相反； 输入0，期待返回0； 输入非数值类型，比如None、[]、{}，期待抛出TypeError。 把上面的测试用例放到一个测试模块里，就是一个完整的单元测试。 如果单元测试通过，说明我们测试的这个函数能够正常工作。如果单元测试不通过，要么函数有bug，要么测试条件输入不正确，总之，需要修复使单元测试能够通过。 单元测试通过后有什么意义呢？如果我们对abs()函数代码做了修改，只需要再跑一遍单元测试，如果通过，说明我们的修改不会对abs()函数原有的行为造成影响，如果测试不通过，说明我们的修改与原有行为不一致，要么修改代码，要么修改测试。 这种以测试为驱动的开发模式最大的好处就是确保一个程序模块的行为符合我们设计的测试用例。在将来修改的时候，可以极大程度地保证该模块行为仍然是正确的。 我们来编写一个Dict类，这个类的行为和dict一致，但是可以通过属性来访问，用起来就像下面这样： &gt;&gt;&gt; d = Dict(a=1, b=2) &gt;&gt;&gt; d[&apos;a&apos;] 1 &gt;&gt;&gt; d.a 1 mydict.py代码如下： class Dict(dict): def __init__(self, **kw): super().__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&quot;&apos;Dict&apos; object has no attribute &apos;%s&apos;&quot; % key) def __setattr__(self, key, value): self[key] = value 为了编写单元测试，我们需要引入Python自带的unittest模块，编写mydict_test.py如下： import unittest from mydict import Dict class TestDict(unittest.TestCase): def test_init(self): d = Dict(a=1, b=&apos;test&apos;) self.assertEqual(d.a, 1) self.assertEqual(d.b, &apos;test&apos;) self.assertTrue(isinstance(d, dict)) def test_key(self): d = Dict() d[&apos;key&apos;] = &apos;value&apos; self.assertEqual(d.key, &apos;value&apos;) def test_attr(self): d = Dict() d.key = &apos;value&apos; self.assertTrue(&apos;key&apos; in d) self.assertEqual(d[&apos;key&apos;], &apos;value&apos;) def test_keyerror(self): d = Dict() with self.assertRaises(KeyError): value = d[&apos;empty&apos;] def test_attrerror(self): d = Dict() with self.assertRaises(AttributeError): value = d.empty 编写单元测试时，我们需要编写一个测试类，从unittest.TestCase继承。 以test开头的方法就是测试方法，不以test开头的方法不被认为是测试方法，测试的时候不会被执行。 对每一类测试都需要编写一个test_xxx()方法。由于unittest.TestCase提供了很多内置的条件判断，我们只需要调用这些方法就可以断言输出是否是我们所期望的。最常用的断言就是assertEqual()： self.assertEqual(abs(-1), 1) # 断言函数返回的结果与1相等 另一种重要的断言就是期待抛出指定类型的Error，比如通过d[‘empty’]访问不存在的key时，断言会抛出KeyError： with self.assertRaises(KeyError): value = d[&apos;empty&apos;] 而通过d.empty访问不存在的key时，我们期待抛出AttributeError： with self.assertRaises(AttributeError): value = d.empty 运行单元测试一旦编写好单元测试，我们就可以运行单元测试。最简单的运行方式是在mydict_test.py的最后加上两行代码： if __name__ == &apos;__main__&apos;: unittest.main() 这样就可以把mydict_test.py当做正常的python脚本运行： $ python mydict_test.py 另一种方法是在命令行通过参数-m unittest直接运行单元测试： $ python -m unittest mydict_test ..... ---------------------------------------------------------------------- Ran 5 tests in 0.000s OK 这是推荐的做法，因为这样可以一次批量运行很多单元测试，并且，有很多工具可以自动来运行这些单元测试。 setUp与tearDown可以在单元测试中编写两个特殊的setUp()和tearDown()方法。这两个方法会分别在每调用一个测试方法的前后分别被执行。 setUp()和tearDown()方法有什么用呢？设想你的测试需要启动一个数据库，这时，就可以在setUp()方法中连接数据库，在tearDown()方法中关闭数据库，这样，不必在每个测试方法中重复相同的代码： class TestDict(unittest.TestCase): def setUp(self): print(&apos;setUp...&apos;) def tearDown(self): print(&apos;tearDown...&apos;) 可以再次运行测试看看每个测试方法调用前后是否会打印出setUp…和tearDown…。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"单元测试","slug":"单元测试","permalink":"http://yoursite.com/tags/单元测试/"}]},{"title":"python之property","slug":"property","date":"2017-03-26T16:00:00.000Z","updated":"2018-03-06T12:57:30.047Z","comments":true,"path":"2017/03/27/property/","link":"","permalink":"http://yoursite.com/2017/03/27/property/","excerpt":"","text":"在工作中，在绑定属性的时候，如果我们直接把属性写成全局的，虽然写起来比较简单，但是容易被随便修改，不符合逻辑，这时就需要把该属性进行限制，改成私有属性，通过内部方法进行调用或者修改以及检测 例子： class Test(object): def __init__(self): self.__num = 100 def getNum(self): return self.__num def setNum(self,newNum): if not isinstance(newNum, int): raise ValueError(&apos;该值必须是一个整数！&apos;) if newNum &lt; 0 or newNum &gt; 300: raise ValueError(&apos;该值必须在0 ~ 300!&apos;) self.__num = newNum t = Test() t.getNum()#获取该值 t.setNum(300)#设置该值 上面这种方式有点繁琐，python还有更好的方式，通过属性的方式 class Test(object): def __init__(self): self.__num = 100 def getNum(self): return self.__num def setNum(self,newNum): if not isinstance(newNum, int): raise ValueError(&apos;该值必须是一个整数！&apos;) if newNum &lt; 0 or newNum &gt; 300: raise ValueError(&apos;该值必须在0 ~ 300!&apos;) self.__num = newNum #只需要在这里进行改写 #这里python会自动检测是设置或是获取，如果获取，则执行getNum，如果是设置，则执行setNum，相当于对该方法进行了简单的封装 #这里需要注意的是：获取一定要放在前面，顺序混淆会报错 num = property(getNum,setNum) t = Test() #可以通过这两个方法进行设置和获取以及设置的时候进行验证 #这是只要通过获取或者设置属性的方式就可以实现和上面一样的功能 t.num #等价于上面的t.getNum() t.num = 300 #等价于上面的t.getNum(300) 在这里再介绍一种property的使用方式,通过装饰器的方法进行实现 class Test(object): def __init__(self): self.__num = 100 #通过装饰器的方式进行实现 #这里等价于上面的，getNum()方法 @property def num(self): return self.__num #这里等价于上面的setNum()方法 @num.setter def num(self,newNum): if not isinstance(newNum, int): raise ValueError(&apos;该值必须是一个整数！&apos;) if newNum &lt; 0 or newNum &gt; 300: raise ValueError(&apos;该值必须在0 ~ 300!&apos;) self.__num = newNum 注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性： class Test(object): def __init__(self): self.__num = 100 @property def num(self): return self.__num @num.setter def num(self,newNum): if not isinstance(newNum, int): raise ValueError(&apos;该值必须是一个整数！&apos;) if newNum &lt; 0 or newNum &gt; 300: raise ValueError(&apos;该值必须在0 ~ 300!&apos;) self.__num = newNum @prorerty def oldNum(self): return 999-self.__num 上面的num是可读写属性，而oldNum就是一个只读属性。 总结：Python内置的@property装饰器就是负责把一个方法变成属性调用的：@property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@num.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作。 @property广泛应用在类的定义中，可以让调用者写出简短的代码，同时保证对参数进行必要的检查，这样，程序运行时就减少了出错的可能性。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"property","slug":"property","permalink":"http://yoursite.com/tags/property/"}]},{"title":"python之装饰器","slug":"装饰器","date":"2017-03-23T16:00:00.000Z","updated":"2018-03-06T12:56:49.285Z","comments":true,"path":"2017/03/24/装饰器/","link":"","permalink":"http://yoursite.com/2017/03/24/装饰器/","excerpt":"","text":"装饰器的作用在原来代码的基础上进行功能扩展 例如：原来我们有2个方法，f1和f2 def f1(): print(&apos;f1&apos;) def f2(): print(&apos;f2&apos;) f1() f2() 现在我们需要给这两个方法调用前加一个验证，验证成功就调用，失败则不调用：首先我们使用一种普通方法： def w1(func): def inner(): print(‘正在验证中’) if True: func() else: print(‘验证失败，请重试’) return inner def f1(): print(&apos;f1&apos;) def f2(): print(&apos;f2&apos;) x = w1(f1) x() y = w1(f2) y() 这种方式基本实现了所需要的验证功能 接下来我们使用通过用装饰器实现的写法： def w1(func): def inner(): print(‘正在验证’) if True: func() else: print(‘验证失败，请重试’) return inner @w1 #相当于w1(f1) def f1(): print(&apos;f1&apos;) @w1 #相当于w1(f2) def f2(): print(&apos;f2&apos;) f1() f2() 类装饰器 class Test(object): def __init__(self,fn): print(&apos;初始化&apos;) print(&apos;fn的名字是{0}&apos;.format(fn.__name__)) self.__fn = fn def __call__(self): print(&apos;---装饰器中的功能---&apos;) self.__fn() @Test #这里的这一步就相当于 # test = Test(test) #也就是实例化一个对象，实例化的时候会触发__init__函数执行，现在 #test也就等于了一个已经实例化后的对象 #如果这里在调用test(),就会触发类里的__call__方法 def test(): print(&apos;====test====&apos;) test() &gt;&gt;&gt;初始化 &gt;&gt;&gt;fn的名字是test &gt;&gt;&gt;---装饰器中的功能--- &gt;&gt;&gt;====test====","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"装饰器","slug":"装饰器","permalink":"http://yoursite.com/tags/装饰器/"}]},{"title":"深入python之浅拷贝和深拷贝","slug":"深拷贝和浅拷贝","date":"2017-03-22T16:00:00.000Z","updated":"2018-03-18T13:08:51.890Z","comments":true,"path":"2017/03/23/深拷贝和浅拷贝/","link":"","permalink":"http://yoursite.com/2017/03/23/深拷贝和浅拷贝/","excerpt":"","text":"关于python中的浅拷贝和深拷贝在Python中，首先要讲的就是等号赋值了，其实这种拷贝只是给源对象新增一个标签而已，指向的还是同一个对象： 例：&gt;&gt;&gt;a = [1,2,3] &gt;&gt;&gt;b = a #打印a的内存地址 &gt;&gt;&gt;id(a) 45817096 #打印b的内存地址 &gt;&gt;&gt;id(b) 45817096 #这里的a和b指向的是同一个对象 接下来就是我们要讲的浅拷贝和深拷贝，实现深浅拷贝首先需要引入copy模块: 1：浅拷贝,使用copy模块下的copy()方法实现： &gt;&gt;&gt;import copy &gt;&gt;&gt;a = [1,2,3,4] &gt;&gt;&gt;b = copy.copy(a) &gt;&gt;&gt; id(a) 47818184 &gt;&gt;&gt; id(b) 47856904 # 在这里我们发现b和a的内存地址不同，相当于复制一份新的列表[1,2,3,4]给了b,如果现在修改a的值，b的值不会随之变化： &gt;&gt;&gt; a.append(10) &gt;&gt;&gt; a [1, 2, 3, 4, 10] &gt;&gt;&gt; b [1, 2, 3, 4] # 我们接着往下走： &gt;&gt;&gt; aa = [1,2,3,[&apos;a&apos;,&apos;b&apos;]] &gt;&gt;&gt; bb = copy.copy(aa) &gt;&gt;&gt; id(aa) 47914120 &gt;&gt;&gt; id(bb) 47697096 # 接着我们继续输出列表嵌套内的列表的内存地址 &gt;&gt;&gt; id(aa[3]) 47696776 &gt;&gt;&gt; id(bb[3]) 47696776 # 在这里我们发现内部嵌套的列表的内存地址是相同的，这里就说明了浅拷贝仅仅只是拷贝了父对象，而不会拷贝扶对象内部的子对象 2：深拷贝，使用copy模块下的deepcopy()方法实现： &gt;&gt;&gt;import copy &gt;&gt;&gt;a = [1,2,3,4] &gt;&gt;&gt;b = copy.deepcopy(a) &gt;&gt;&gt; id(a) 47818184 &gt;&gt;&gt; id(b) 47856904 #这里可以看到，如果当前拷贝对象没有子对象的时候，copy()和deep()没有区别 #接着往下看： &gt;&gt;&gt; a = [1,2,3,4,[&apos;x&apos;,&apos;y&apos;]] &gt;&gt;&gt; b = copy.deepcopy() &gt;&gt;&gt; id(a[4]) 47733896 &gt;&gt;&gt; id(b[4]) 47914056 #这里我们发现，深拷贝会拷贝当前对象及其子对象，不论父对象嵌套多少子对象都会进行完全拷贝 我们发现，刚才举例的时候只是用到了列表，实际工作中用到的不止有列表这么一种数据类型，我们不妨试一下其他的类型，比如试一试元祖： &gt;&gt;&gt; a = (1,2,3,4,[&apos;aa&apos;,&apos;bb&apos;]) &gt;&gt;&gt; b = copy.copy(a) &gt;&gt;&gt; id(a) 45195112 &gt;&gt;&gt; id(b) 45195112 &gt;&gt;&gt; id(a[4]) 46124808 &gt;&gt;&gt; id(b[4]) 46124808 #这里我们发现浅拷贝元祖的时候父对象也没有实现拷贝，还是指向了原来对象的内存地址 &gt;&gt;&gt; c = copy.deepcopy(a) &gt;&gt;&gt; id(c) 45853160 &gt;&gt;&gt; id(c[4]) 46169224 &gt;&gt;&gt; id(a[4]) 46124808 #而深拷贝还是一如既往的一致，父对象和子对象都进行了拷贝 #其实这里python默认帮我们进行了类型检测，如果是不可变类型则无法进行浅拷贝，只可以执行深拷贝，这个需要重点记忆，工作中可能就莫名出现此类BUG； 小结： copy.copy() 浅拷贝 只拷贝父对象，不会拷贝对象的内部的子对象。 copy.deepcopy() 深拷贝 拷贝对象及其子对象 如果拷贝的类型是不可变类型的时候，浅拷贝不会拷贝父对象，只有深拷贝会执行拷贝","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"模块","slug":"模块","permalink":"http://yoursite.com/tags/模块/"},{"name":"copy","slug":"copy","permalink":"http://yoursite.com/tags/copy/"}]},{"title":"python之内建函数","slug":"内建函数","date":"2017-03-21T16:00:00.000Z","updated":"2018-03-06T12:56:40.096Z","comments":true,"path":"2017/03/22/内建函数/","link":"","permalink":"http://yoursite.com/2017/03/22/内建函数/","excerpt":"","text":"1，map函数 定义：map函数会根据提供的函数对指定序列做映射，生成新的列表 map(function,sequence[,sequence,…]) &gt;&gt;list function是一个函数sequence是一个或者多个学列，取决于function需要几个参数python3中返回的是map对象，需要迭代才能获取对象中的值，可以在外面转换成list对象 参数学列中的每一个元素分别调用function函数，返回包含每次function函数返回的map对象 例1： &gt;&gt;&gt; k = map(lambda x:x*x, [1,2,3]) &gt;&gt;&gt; k &gt;&gt;&gt; for i in k: print(i) 1 4 9 &gt;&gt;&gt; k =list(map(lambda x:x*x, [1,2,3])) &gt;&gt;&gt; k 1 4 9 例2： #函数需要2个参数的情况 &gt;&gt;&gt; m = list(map(lambda x,y:x+y,[1,2,3],[4,5,6])) &gt;&gt;&gt; m [5, 7, 9] 例3：上面两个例子都是用匿名函数的方式，下面来看不使用匿名函数的方式： &gt;&gt;&gt; def f1(x,y): return (x,y) &gt;&gt;&gt; a1 = [1,2,3,4,5,6] &gt;&gt;&gt; a2 = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;,&apos;f&apos;] &gt;&gt;&gt; list(map(f1,a1,a2)) [(1, &apos;a&apos;), (2, &apos;b&apos;), (3, &apos;c&apos;), (4, &apos;d&apos;), (5, &apos;e&apos;), (6, &apos;f&apos;)] 2，filter 定义：filter函数会对制定序列执行过滤、筛选操作 filter(function or None,squence)&gt;&gt;&gt;list,tuple,or string function:接受一个参数，返回布尔值True或False sequence:序列可以是str,tuple,list filter函数会对序列参数sequence中的每个元素调用function函数，最后返回的结果包含调用结果为True的元素。返回值的类型和参数sequence的类型相同 例1： &gt;&gt;&gt; list(filter(lambda x:x%2,[1,2,3,4,5,6,7,8])) [1, 3, 5, 7] 3，sorted函数 定义：对参数进行排序返回新的列表，如果是字母则通过ASII码进行排序 例1：正序排序 &gt;&gt;&gt; sorted([1,2,3,5,3,12,4,1234,34,343]) [1, 2, 3, 3, 4, 5, 12, 34, 343, 1234] 例2：倒序排序 &gt;&gt;&gt; sorted([4,66,1,65,23,8,5,],reverse = True) [66, 65, 23, 8, 5, 4, 1] 例3：字母排序 &gt;&gt;&gt; sorted([&apos;a&apos;,&apos;c&apos;,&apos;e&apos;,&apos;b&apos;,&apos;f&apos;]) [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;e&apos;, &apos;f&apos;] #如果是单词的话会通过它们首字母的asii码进行排序 &gt;&gt;&gt; sorted([&apos;hello&apos;,&apos;world&apos;,&apos;name&apos;,&apos;args&apos;]) [&apos;args&apos;, &apos;hello&apos;, &apos;name&apos;, &apos;world&apos;]","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"内建函数","slug":"内建函数","permalink":"http://yoursite.com/tags/内建函数/"}]},{"title":"python之垃圾回收","slug":"垃圾回收","date":"2017-03-20T16:00:00.000Z","updated":"2018-03-06T12:56:37.104Z","comments":true,"path":"2017/03/21/垃圾回收/","link":"","permalink":"http://yoursite.com/2017/03/21/垃圾回收/","excerpt":"","text":"垃圾回收机制（GC） 关于垃圾回收高级语言里都采用了垃圾回收机制，而不再是C，C++里用户自己管理内存。在python里采用的是引用技术机制为主，标记-清除和分代手机两种机制为辅的策略 关于引用计数python里每个东西都是对象，通过底层的一个方法对当前对象的引用进行计数,当一个对象有新的引用的时候，它的计数就会增加，当引用它的对象被删除，它的引用计数就减少，当引用计数为0的时候，该对象生命就结束了。 优点：1，简单2，实时性：一旦没有引用，内存就直接释放了。不用想其他机制等待特定时机，实时性带来的另外一个好处，处理回收内存的时间分摊了，不会等到特定时间做特定的事情。 缺点：1，维护引用计数消耗资源2，循环引用的问题，已经通过隔代回收的方式进行了处理，首先创建对象之后在特定时间会进行检测2个对象之间是否在进行相互引用，如果是则进行回收。然后把没被清除的对象移动到下个数据链中。然后继续进行在特定时间进行检测。 更底层的进行垃圾回收管理需要引入gc模块进行管理，这里就不进行深入理解了。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"http://yoursite.com/tags/垃圾回收/"}]},{"title":"python之进程的创建","slug":"进程的创建fork","date":"2017-03-19T16:00:00.000Z","updated":"2018-03-06T12:56:32.547Z","comments":true,"path":"2017/03/20/进程的创建fork/","link":"","permalink":"http://yoursite.com/2017/03/20/进程的创建fork/","excerpt":"","text":"1，进程和程序 编写完毕的代码，在没有运行的时候，称之为程序正在运行着的代码，就成为进程进程，除了包含代码以外，还有需要运行的环境等，所以和程序是有区别的 2，fork()","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"进程","slug":"进程","permalink":"http://yoursite.com/tags/进程/"}]},{"title":"python之集合","slug":"集合","date":"2017-03-16T16:00:00.000Z","updated":"2018-03-01T14:03:21.079Z","comments":true,"path":"2017/03/17/集合/","link":"","permalink":"http://yoursite.com/2017/03/17/集合/","excerpt":"","text":"定义：python的set和其他语言类似, 是一个无序不重复元素集, 基本功能包括关系测试和消除重复元素. 集合对象还支持union(联合), intersection(交), difference(差)和sysmmetric difference(对称差集)等数学运算. sets 支持 x in set, len(set),和 for x in set。作为一个无序的集合，sets不记录元素位置或者插入点。因此，sets不支持 indexing, slicing, 或其它类序列（sequence-like）的操作。 1，集合功能之一：列表去重因为集合是无序不重复元素集，利用这一特性可以进行列表的去重操作例： &gt;&gt;&gt; a = [11,23,345,657,453,123,11,23,345,657,453,123] &gt;&gt;&gt; b = set(a) &gt;&gt;&gt; b {453, 11, 657, 23, 345, 123} &gt;&gt;&gt; a = list(b) &gt;&gt;&gt; a [453, 11, 657, 23, 345, 123] 2，集合功能之二：交集 &gt;&gt;&gt; a = {&apos;a&apos;,&apos;b&apos;,&apos;d&apos;,&apos;f&apos;,&apos;l&apos;,&apos;k&apos;,&apos;m&apos;} &gt;&gt;&gt; b = {&apos;k&apos;,&apos;s&apos;,&apos;q&apos;,&apos;k&apos;,&apos;y&apos;,&apos;z&apos;,&apos;b&apos;} &gt;&gt;&gt; a&amp;b {&apos;b&apos;, &apos;k&apos;} 3，集合功能之三：并集 &gt;&gt;&gt; a = {&apos;a&apos;,&apos;b&apos;,&apos;d&apos;,&apos;f&apos;,&apos;l&apos;,&apos;k&apos;,&apos;m&apos;} &gt;&gt;&gt; b = {&apos;k&apos;,&apos;s&apos;,&apos;q&apos;,&apos;k&apos;,&apos;y&apos;,&apos;z&apos;,&apos;b&apos;} &gt;&gt;&gt; a | b {&apos;s&apos;, &apos;k&apos;, &apos;q&apos;, &apos;m&apos;, &apos;z&apos;, &apos;l&apos;, &apos;f&apos;, &apos;a&apos;, &apos;d&apos;, &apos;y&apos;, &apos;b&apos;} 4，集合功能之四：差集 &gt;&gt;&gt; a = {&apos;a&apos;,&apos;b&apos;,&apos;d&apos;,&apos;f&apos;,&apos;l&apos;,&apos;k&apos;,&apos;m&apos;} &gt;&gt;&gt; b = {&apos;k&apos;,&apos;s&apos;,&apos;q&apos;,&apos;k&apos;,&apos;y&apos;,&apos;z&apos;,&apos;b&apos;} &gt;&gt;&gt; a-b #a中有的而b中没有的 {&apos;m&apos;, &apos;l&apos;, &apos;f&apos;, &apos;a&apos;, &apos;d&apos;} 5，集合功能之五：对称差集 &gt;&gt;&gt; a = {&apos;a&apos;,&apos;b&apos;,&apos;d&apos;,&apos;f&apos;,&apos;l&apos;,&apos;k&apos;,&apos;m&apos;} &gt;&gt;&gt; b = {&apos;k&apos;,&apos;s&apos;,&apos;q&apos;,&apos;k&apos;,&apos;y&apos;,&apos;z&apos;,&apos;b&apos;} &gt;&gt;&gt; a^b #a有的，b没有的，b有的，a没有的 {&apos;s&apos;, &apos;q&apos;, &apos;z&apos;, &apos;m&apos;, &apos;l&apos;, &apos;f&apos;, &apos;a&apos;, &apos;y&apos;, &apos;d&apos;}","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"set集合","slug":"set集合","permalink":"http://yoursite.com/tags/set集合/"}]},{"title":"python之对象池","slug":"对象池","date":"2017-03-15T16:00:00.000Z","updated":"2018-03-01T14:00:27.140Z","comments":true,"path":"2017/03/16/对象池/","link":"","permalink":"http://yoursite.com/2017/03/16/对象池/","excerpt":"","text":"1、小整数对象池整数在程序中的使用非常广泛，Python为了优化速度，使用了小整数对象池，避免为整数频繁申请和销毁内存空间。python对小整数的定义是[-5,257],也就是-5到256且包含256，这些整数对象是提前建立好的，不会被垃圾回收，在一个python的程序中，所有位于这个范围内的整数使用的都是同一个对象。同理，单个字母也是这样的。但是当定义2个相同的字符串的时候，引用计数为0，触发垃圾回收 2、大整数对象池（什么时候使用，什么时候创建，小整数池之外的整数）每一个大整数，均创建一个新的对象 3、intern机制 如果定义的变量都是普通字母，不包含其他特殊符号的，默认开启intern机制，共用对象，不会进行新建 总结： 小整数[-5,257]共用对象，常驻内存 单个字符共用对象，常驻内存 单个单词，不可修改，默认开启intern机制，共用对象，引用计数为0，则销毁 字符串（含有空格），不可修改，不开启intern机制，不共用对象，引用计数为0，则销毁","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"对象池","slug":"对象池","permalink":"http://yoursite.com/tags/对象池/"}]},{"title":"python之动态添加属性方法","slug":"动态添加属性和方法","date":"2017-03-14T16:00:00.000Z","updated":"2018-03-01T13:58:18.517Z","comments":true,"path":"2017/03/15/动态添加属性和方法/","link":"","permalink":"http://yoursite.com/2017/03/15/动态添加属性和方法/","excerpt":"","text":"工作中，我们需要给某一个实例对象新添加一个新的实例方法，如果用添加属性的方法进行添加的话直接会报错： class Person: def __init__(self,newName,newAge): self.name = newName self.age = newAge def eat(self): print(&apos;==={0}正在吃&apos;.format(self.name)) p1 = Person(&apos;p1&apos;,20) p1.eat() def run(self): print(&apos;===={0}正在跑&apos;.format(self.name)) p1.run = run #虽然p1对象中run属性指向了run函数，但是因为run属性指向的函数是后来添加的，调用的时候并没有把p1这个属性传进去，所以会报参数错误 其实python提供了一个方法供我们动态的给对象添加一个方法： 这时候我们可以通过一个types模块里的方法实现动态添加方法 import types 这里的第一个参数是要添加的方法，第二个参数是要添加的对象然后把它赋给要绑定的这个对象的方法 p1.run = types.MethodType(run,p1) 这时候就可以直接调用新添加的方法了 p1.run() 注意：只有实例对象动态添加一个方法的时候需要types.MethodType方法而给类添加静态方法和类方法则不需要这种写法 #添加类方法 @classmethod def printNum(cls): print(&apos;classMethod&apos;) P1.printNum = printNum P1.printNum() #添加静态方法 @staticmethod def test(): print(&apos;静态方法&apos;) P1.test = test P1.test() 如果我们想要显示实例的属性怎么办呢？比如我们只允许是实例对象添加name和age属性，为了达到限定的目的，python允许在定义class的时候定义一个特殊的slots变量，来限制该class实例对象可以添加的属性： 注：这个变量的值必须放在一个元祖中 class Per(object): __slots__ = (&apos;name&apos;,&apos;age&apos;) #用tuple定义允许绑定的属性名称 p = Per() p.name = &apos;呵呵&apos; p.age = 111 print(p.name) print(p.age) p.add = &apos;北京&apos;#这里就会报错，找不到p.add这个属性 print(p.add)","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"动态添加属性","slug":"动态添加属性","permalink":"http://yoursite.com/tags/动态添加属性/"}]},{"title":"python迭代、列表生成式和生成器","slug":"迭代、列表生成式和生成器","date":"2017-03-13T16:00:00.000Z","updated":"2018-03-18T13:08:23.473Z","comments":true,"path":"2017/03/14/迭代、列表生成式和生成器/","link":"","permalink":"http://yoursite.com/2017/03/14/迭代、列表生成式和生成器/","excerpt":"","text":"转载自廖雪峰官网：1,迭代 如果给定一个list或tuple等可迭代对象，我们可以通过for循环来遍历这些可迭代对象，这种遍历我们称为迭代 在python中，迭代是通过for in来完成的。 list这种数据类型虽然有下标，但很多其他数据类型是没有下标的，但是，只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代： dicts = {&apos;a&apos;:1,&apos;b&apos;:2,&apos;c&apos;:3} #这里我们迭代的是字典的键 for key in dicts: print(key) #如果要迭代字典的值，可以用： for val in dicts.values(): print(val) 或者： for key in dicts: print(dicts[key]) #如果要同时获取键和值，则可以： for key,val in dicts.items(): print(key,val) 或者： for key in dicts: print(key,dicts[key]) 所以，当我们使用for循环时，只要作用于一个可迭代对象，for循环就可以正常运行，而我们不太关心该对象究竟是list还是其他数据类型。 那么，如何判断一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断： &gt;&gt;&gt; from collections import Iterable &gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable) # str是否可迭代 True &gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代 True &gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代 False 最后一个小问题，如果要对list实现类似下标循环怎么办？Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： &gt;&gt;&gt; for i, value in enumerate([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]): ... print(i, value) ... 0 A 1 B 2 C &gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]: ... print(x, y) ... 1 1 2 4 3 9 小结： 任何可迭代对象都可以作用于for循环，包括我们自定义的数据类型，只要符合迭代条件，就可以使用for循环。 2，列表生成式： 列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。 举个例子，要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))： &gt;&gt;&gt; list(range(1, 11)) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 但如果要生成[1x1, 2x2, 3x3, …, 10x10]怎么做？方法一是循环： &gt;&gt;&gt; L = [] &gt;&gt;&gt; for x in range(1, 11): ... L.append(x * x) ... &gt;&gt;&gt; L [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list： &gt;&gt;&gt; [x * x for x in range(1, 11)] [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法。 for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： &gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0] [4, 16, 36, 64, 100]： 还可以使用两层循环，可以生成全排列： &gt;&gt;&gt; [m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;] [&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;] 三层和三层以上的循环就很少用到了。 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现： &gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到 &gt;&gt;&gt; [d for d in os.listdir(&apos;.&apos;)] # os.listdir可以列出文件和目录 [&apos;.emacs.d&apos;, &apos;.ssh&apos;, &apos;.Trash&apos;, &apos;Adlm&apos;, &apos;Applications&apos;, &apos;Desktop&apos;, &apos;Documents&apos;, &apos;Downloads&apos;, &apos;Library&apos;, &apos;Movies&apos;, &apos;Music&apos;, &apos;Pictures&apos;, &apos;Public&apos;, &apos;VirtualBox VMs&apos;, &apos;Workspace&apos;, &apos;XCode&apos;] for循环其实可以同时使用两个甚至多个变量，比如dict的items()可以同时迭代key和value： &gt;&gt;&gt; d = {&apos;x&apos;: &apos;A&apos;, &apos;y&apos;: &apos;B&apos;, &apos;z&apos;: &apos;C&apos; } &gt;&gt;&gt; for k, v in d.items(): ... print(k, &apos;=&apos;, v) ... y = B x = A z = C 因此，列表生成式也可以使用两个变量来生成list： &gt;&gt;&gt; d = {&apos;x&apos;: &apos;A&apos;, &apos;y&apos;: &apos;B&apos;, &apos;z&apos;: &apos;C&apos; } &gt;&gt;&gt; [k + &apos;=&apos; + v for k, v in d.items()] [&apos;y=B&apos;, &apos;x=A&apos;, &apos;z=C&apos;] 最后把一个list中所有的字符串变成小写： &gt;&gt;&gt; L = [&apos;Hello&apos;, &apos;World&apos;, &apos;IBM&apos;, &apos;Apple&apos;] &gt;&gt;&gt; [s.lower() for s in L] [&apos;hello&apos;, &apos;world&apos;, &apos;ibm&apos;, &apos;apple&apos;] 3,生成器 通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： &gt;&gt;&gt; L = [x * x for x in range(10)] &gt;&gt;&gt; L [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] &gt;&gt;&gt; g = (x * x for x in range(10)) &gt;&gt;&gt; g &lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。 我们可以直接打印出list的每一个元素，但我们怎么打印出generator的每一个元素呢？ 如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值,也可以使用next方式来获取：例子：next(g) == g.next() &gt;&gt;&gt; next(g) 0 &gt;&gt;&gt; next(g) 1 &gt;&gt;&gt; next(g) 4 &gt;&gt;&gt; next(g) 9 &gt;&gt;&gt; next(g) 16 &gt;&gt;&gt; next(g) 25 &gt;&gt;&gt; next(g) 36 &gt;&gt;&gt; next(g) 49 &gt;&gt;&gt; next(g) 64 &gt;&gt;&gt; next(g) 81 &gt;&gt;&gt; next(g) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; StopIteration 所以，我们创建了一个generator后，基本上永远不会调用next()，而是通过for循环来迭代它，并且不需要关心StopIteration的错误。 generator非常强大。如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现。 比如，著名的斐波拉契数列（Fibonacci），除第一个和第二个数外，任意一个数都可由前两个数相加得到： 1, 1, 2, 3, 5, 8, 13, 21, 34, … 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return ‘done’注意，赋值语句： a, b = b, a + b相当于： t = (b, a + b) # t是一个tuplea = t[0]b = t[1]但不必显式写出临时变量t就可以赋值。 上面的函数可以输出斐波那契数列的前N个数： fib(6)112358‘done’仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return ‘done’这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator，也就是一个生成器： f = fib(6)f打印结果是一个生成器，而且用next()每次执行这个生成器的时候，会在遇到yield的时候进行终止，然后返回yield后面的值，等待下次的调用。 这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 举个简单的例子，定义一个generator，依次返回数字1，3，5： def odd(): print(‘step 1’) yield 1 print(‘step 2’) yield(3) print(‘step 3’) yield(5)调用该generator时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值： &gt;&gt;&gt; o = odd() &gt;&gt;&gt; next(o) step 1 1 &gt;&gt;&gt; next(o) step 2 3 &gt;&gt;&gt; next(o) step 3 5 &gt;&gt;&gt; next(o) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; StopIteration 可以看到，odd不是普通函数，而是generator，在执行过程中，遇到yield就中断，下次又继续执行。执行3次yield后，已经没有yield可以执行了，所以，第4次调用next(o)就报错。 回到fib的例子，我们在循环过程中不断调用yield，就会不断中断。当然要给循环设置一个条件来退出循环，不然就会产生一个无限数列出来。 同样的，把函数改成generator后，我们基本上从来不会用next()来获取下一个返回值，而是直接使用for循环来迭代： &gt;&gt;&gt; for n in fib(6): ... print(n) ... 1 1 2 3 5 8 但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中： &gt;&gt;&gt; g = fib(6) &gt;&gt;&gt; while True: ... try: ... x = next(g) ... print(&apos;g:&apos;, x) ... except StopIteration as e: ... print(&apos;Generator return value:&apos;, e.value) ... break ... g: 1 g: 1 g: 2 g: 3 g: 5 g: 8 Generator return value: done send方法的使用：从下面这个例子可以看出，send的方法使用起来和next功能差不多，只不过send方法会顺便给yield i这个整体一个返回值，剩下的功能就和next一样了注意：使用send（）传值的时候不能在第一次使用，这样会报错，如果第一次就要使用send,则给它传一个Nonesend(None) &gt;&gt;&gt; def test(): i = 0 while i&lt;5: temp = yield i print(temp) i+=1 &gt;&gt;&gt; test() &lt;generator object test at 0x0000000002DAA4C0&gt; &gt;&gt;&gt; t = test() &gt;&gt;&gt; t.__next__() 0 &gt;&gt;&gt; t.__next__() None 1 &gt;&gt;&gt; next(t) None 2 &gt;&gt;&gt; t.send(&apos;hehe&apos;) hehe 3","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"迭代","slug":"迭代","permalink":"http://yoursite.com/tags/迭代/"},{"name":"列表生成式","slug":"列表生成式","permalink":"http://yoursite.com/tags/列表生成式/"},{"name":"生成器","slug":"生成器","permalink":"http://yoursite.com/tags/生成器/"},{"name":"yieid","slug":"yieid","permalink":"http://yoursite.com/tags/yieid/"}]},{"title":"python之闭包","slug":"闭包","date":"2017-03-12T16:00:00.000Z","updated":"2018-03-06T13:43:17.384Z","comments":true,"path":"2017/03/13/闭包/","link":"","permalink":"http://yoursite.com/2017/03/13/闭包/","excerpt":"","text":"如果在一个函数的内部定义了另一个函数，外部的我们叫他外函数，内部的我们叫他内函数。 闭包三要素： 1，在一个外函数中定义了一个内函数， 2，内函数里运用了外函数的临时变量， 3，并且外函数的返回值是内函数的引用。 这样就构成了一个闭包。 一般情况下，在我们认知当中，如果一个函数结束，函数的内部所有东西都会释放掉，还给内存，局部变量都会消失。但是闭包是一种特殊情况，如果外函数在结束的时候发现有自己的临时变量将来会在内部函数中用到，就把这个临时变量绑定给了内部函数，然后自己再结束。 #闭包函数的实例 # outer是外部函数 a和b都是外函数的临时变量 def outer( a ): b = 10 # inner是内函数 def inner(): #在内函数中 用到了外函数的临时变量 print(a+b) # 外函数的返回值是内函数的引用 return inner if __name__ == &apos;__main__&apos;: # 在这里我们调用外函数传入参数5 #此时外函数两个临时变量 a是5 b是10 ，并创建了内函数，然后把内函数的引用返回存给了demo # 外函数结束的时候发现内部函数将会用到自己的临时变量，这两个临时变量就不会释放，会绑定给这个内部函数 demo = outer(5) # 我们调用内部函数，看一看内部函数是不是能使用外部函数的临时变量 # demo存了外函数的返回值，也就是inner函数的引用，这里相当于执行inner函数 demo() # 15 demo2 = outer(7) demo2()#17 1 外函数返回了内函数的引用： 引用是什么？在python中一切都是对象，包括整型数据1，函数，其实是对象。 当我们进行a=1的时候，实际上在内存当中有一个地方存了值1，然后用a这个变量名存了1所在内存位置的引用。大家可以把引用理解成地址。a只不过是一个变量名字，a里面存的是1这个数值所在的地址，就是a里面存了数值1的引用。 相同的道理，当我们在python中定义一个函数def demo(): 的时候，内存当中会开辟一些空间，存下这个函数的代码、内部的局部变量等等。这个demo只不过是一个变量名字，它里面存了这个函数所在位置的引用而已。我们还可以进行x = demo， y = demo， 这样的操作就相当于，把demo里存的东西赋值给x和y，这样x 和y 都指向了demo函数所在的引用，在这之后我们可以用x() 或者 y() 来调用我们自己创建的demo() ，调用的实际上根本就是一个函数，x、y和demo三个变量名存了同一个函数的引用。 不知道大家有没有理解，很晦涩，希望我说明白了我想表达的。 有了上面的解释，我们可以继续说，返回内函数的引用是怎么回事了。对于闭包，在外函数outer中 最后return inner，我们在调用外函数 demo = outer() 的时候，outer返回了inner，inner是一个函数的引用，这个引用被存入了demo中。所以接下来我们再进行demo() 的时候，相当于运行了inner函数。 同时我们发现，一个函数，如果函数名后紧跟一对括号，相当于现在我就要调用这个函数，如果不跟括号，相当于只是一个函数的名字，里面存了函数所在位置的引用。 2 外函数把临时变量绑定给内函数： 按照我们正常的认知，一个函数结束的时候，会把自己的临时变量都释放还给内存，之后变量都不存在了。一般情况下，确实是这样的。但是闭包是一个特别的情况。外部函数发现，自己的临时变量会在将来的内部函数中用到，自己在结束的时候，返回内函数的同时，会把外函数的临时变量送给内函数绑定在一起。所以外函数已经结束了，调用内函数的时候仍然能够使用外函数的临时变量。 在我编写的实例中，我两次调用外部函数outer,分别传入的值是5和7。内部函数只定义了一次，我们发现调用的时候，内部函数是能识别外函数的临时变量是不一样的。python中一切都是对象，虽然函数我们只定义了一次，但是外函数在运行的时候，实际上是按照里面代码执行的，外函数里创建了一个函数，我们每次调用外函数，它都创建一个内函数，虽然代码一样，但是却创建了不同的对象，并且把每次传入的临时变量数值绑定给内函数，再把内函数引用返回。虽然内函数代码是一样的，但其实，我们每次调用外函数，都返回不同的实例对象的引用，他们的功能是一样的，但是它们实际上不是同一个函数对象。 闭包中内函数修改外函数局部变量： 在闭包内函数中，我们可以随意使用外函数绑定来的临时变量，但是如果我们想修改外函数临时变量数值的时候发现出问题了！ 在基本的python语法当中，一个函数可以随意读取全局数据，但是要修改全局数据的时候有两种方法:1 global 声明全局变量 2 全局变量是可变类型数据的时候可以修改 在闭包内函数也是类似的情况。在内函数中想修改闭包变量（外函数绑定给内函数的局部变量）的时候： 1 在python3中，可以用nonlocal 关键字声明 一个变量， 表示这个变量不是局部变量空间的变量，需要向上一层变量空间找这个变量。 2 在python2中，没有nonlocal这个关键字，我们可以把闭包变量改成可变类型数据进行修改，比如列表。 #修改闭包变量的实例 # outer是外部函数 a和b都是外函数的临时变量 def outer( a ): b = 10 # a和b都是闭包变量 c = [a] #这里对应修改闭包变量的方法2 # inner是内函数 def inner(): #内函数中想修改闭包变量 # 方法1 nonlocal关键字声明 nonlocal b b+=1 # 方法二，把闭包变量修改成可变数据类型 比如列表 c[0] += 1 print(c[0]) print(b) # 外函数的返回值是内函数的引用 return inner if __name__ == &apos;__main__&apos;: demo = outer(5) demo() # 6 11 从上面代码中我们能看出来，在内函数中，分别对闭包变量进行了修改，打印出来的结果也确实是修改之后的结果。以上两种方法就是内函数修改闭包变量的方法。 还有一点需要注意：使用闭包的过程中，一旦外函数被调用一次返回了内函数的引用，虽然每次调用内函数，是开启一个函数执行过后消亡，但是闭包变量实际上只有一份，每次开启内函数都在使用同一份闭包变量 def outer(x): def inner(y): nonlocal x x+=y return x return inner a = outer(10) print(a(1)) //11 print(a(3)) //14 两次分别打印出11和14，由此可见，每次调用inner的时候，使用的闭包变量x实际上是同一个。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"闭包","slug":"闭包","permalink":"http://yoursite.com/tags/闭包/"}]},{"title":"python之常用模块块","slug":"常用模块","date":"2017-03-09T16:00:00.000Z","updated":"2018-03-01T13:42:28.550Z","comments":true,"path":"2017/03/10/常用模块/","link":"","permalink":"http://yoursite.com/2017/03/10/常用模块/","excerpt":"","text":"python有一套很有用的标准库，标准库会随着python解释器，一起安装在你的电脑中的。它是python的一个组成部分，这些标准库是Python为你准备好的利器，可以让编程事半功倍。 标准库 说明 builtins 内建函数默认加载 os 操作系统接口 sys Python自身的运行环境 functools 常用的工具 json 编码和解码JSON对象 logging 记录日志，调试 multiprocessing 多进程 threading 多线程 copy 拷贝 time 时间 datetime 日期和时间 calendar 日历 hashlib 加密算法 random 生成随机数 re 字符串正则匹配 socket 标准的BSD Sockets API shutil 文件和目录管理 glob 基于文件通配符搜索 1，hashlib 加密算法，主要用于注册登录加密 &gt;&gt;&gt; import hashlib &gt;&gt;&gt; t = hashlib.md5() #创建hash对象，md5 &gt;&gt;&gt; t.update(b&apos;yanruilong&apos;) #更新哈希对象传一个字符串参数 &gt;&gt;&gt; t.hexdigest() #返回十六进制数字字符串 &apos;3e0474ae09c74b6276d23eb56b1209da&apos;","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"常用模块","slug":"常用模块","permalink":"http://yoursite.com/tags/常用模块/"}]}]}